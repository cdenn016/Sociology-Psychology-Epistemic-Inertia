\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tcolorbox}
\usepackage{enumitem}
 \usepackage{subcaption} 
\geometry{margin=1in}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\Sig}{\Sigma}
\newcommand{\SigQ}{\Sigma^q}
\newcommand{\SigP}{\Sigma^p}
\newcommand{\muQ}{\mu^q}
\newcommand{\muP}{\mu^p}
\newcommand{\Dmu}{\Delta\mu}
\newcommand{\vech}{\mathrm{vech}}
\newcommand{\Prec}{P}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{prediction}[theorem]{Prediction}

\title{The Inertia of Belief}

\author{
Robert C. Dennis\\
\texttt{cdenn016@gmail.com}
}

\date{\today}

\begin{document}
\maketitle


\begin{abstract}
Phenomenological mass/spring models of belief dynamics have proven empirically successful yet lack theoretical justification. We show that variational free energy minimization on statistical manifolds naturally suggests the Fisher information metric plays the role of an inertial mass-like tensor whereby confident beliefs resist change while uncertain beliefs update readily. Here, we define proper time as an information-theoretic arc length, providing a principled notion of ``how long'' belief updates take which depends on agent precision and their social interactions as encoded via gauge-invariant attention mechanisms.

We present a Hamiltonian formulation of belief dynamics as a fruitful ansatz. In the overdamped limit, which does not depend on this ansatz, we derive several classical sociological models as exact or approximate limiting cases: DeGroot, Friedkin-Johnsen, bounded confidence, echo chambers, Social Impact Theory, and diffusion of innovations. In underdamped regimes however, the ansatz predicts oscillation, overshooting, and resonance as observed by previous researchers. This framework reframes confirmation bias, belief perseverance, and polarization as geometric consequences of epistemic inertia rather than irrationality and unifies multiple theories and regimes of sociological and psychological models.
\end{abstract}


\noindent\textbf{Keywords:} Gauge theory $\cdot$ Active inference $\cdot$ Free energy principle $\cdot$ Information geometry $\cdot$ Sociology

\section{Introduction}

Why do some beliefs resist change more than others? Why does influence itself seem to harden the minds and poison the empathies of those who wield it? Some beliefs are stiff while others readily sway. While confident beliefs clearly possess more "cognitive inertia" than uncertain ones, a principled mathematical foundation for this intuitive phenomenon remains elusive. Current theories of belief updating, from Bayesian inference \citet{jaynes2003probability} to predictive coding \citet{friston2010,clark2013whatever}, model belief change as gradient descent. This is a purely dissipative process where beliefs flow toward lower free energy without momentum, inertia, or dynamics. Though enormously successful across neuroscience \citet{friston2016active}, psychology \citet{hohwy2013predictive}, and machine learning \citet{millidge2021predictive}, this framework does not account for inertial or oscillatory dynamics.

In this article, we show that beliefs possess an epistemic inertia proportional to an agent's prior precision, observational precision, and their social interactions. Just as physical objects with mass resist acceleration, beliefs held with high confidence resist change and, once moving, tend to continue in their direction on a statistical manifold of beliefs. This is not merely metaphor: the second-order expansion of variational free energy reveals that the Fisher information metric \citep{Amari2016} suggests an ansatz for an inertial mass tensor for belief dynamics. The Hamiltonian structure we adopt treats these second-order terms, traditionally neglected \citep{friston2008hierarchical,bogacz2017tutorial}, as generating momentum with sociological and psychological consequences.

Furthermore, beliefs propagate through networks of agents in attention patterns ranging from coordinated consensus to turbulent disagreement, often exhibiting distortion, resonance, and phase transitions \citep{castellano2009statistical,galam2012sociophysics}. While numerous models capture aspects of this collective evolution, from opinion dynamics \citep{hegselmann2002opinion} to quantum-inspired approaches \citep{busemeyer2012quantum}, a principled geometric foundation has been lacking.

As an intuitive example, consider an agent with strong priors about a political position. When presented with contradicting evidence, their belief doesn't immediately flip but resists change, may overshoot when it does shift, and might oscillate before settling. Conversely, an uncertain agent responds quickly to new evidence with minimal resistance. These phenomena, typically attributed to cognitive biases \citep{kahneman2011thinking}, emerge naturally from belief inertia.

Our framework provides three primary contributions to the field:

First, we derive second-order belief dynamics from a first-principles model, showing that the Fisher metric provides a natural inertial mass tensor $M = \Lambda_{\text{prior}} + \Lambda_{\text{observation}} + \Lambda_{\text{social}}^{\text{in}} + \Lambda_{\text{social}}^{\text{out}}$ combining prior conviction, sensory data, and social interactions (both incoming influence and outgoing recoil). Via pullback geometry on statistical manifolds \citep{Amari2016,nielsen2020elementary}, we extend variational free energy to multi-agent systems where social coupling takes the form $D_{\mathrm{KL}}(q_i \| \Omega_{ij} q_j)$, penalizing disagreement with neighbors after gauge transport into a common frame.

Second, we demonstrate that several foundational models from sociology and opinion dynamics emerge as limiting cases of our unified framework. DeGroot social learning \citep{degroot1974}, Friedkin-Johnsen opinion dynamics \citep{friedkin1990social}, bounded confidence models \citep{hegselmann2002opinion,deffuant2000mixing}, echo chamber formation, Social Impact Theory \citep{latane1981psychology}, and diffusion of innovations \citep{rogers2003diffusion} all arise from appropriate parameter regimes and approximations of the same variational free energy functional. This unification reveals that phenomena previously requiring separate theoretical apparatuses---consensus formation, polarization, bounded confidence, and innovation adoption---are manifestations of a single geometric principle operating under different boundary conditions.

Third, we unify documented but theoretically distinct psychological phenomena such as attitude oscillation in persuasion \citet{kaplowitz1992,fink2002}, perceptual overshoot \citet{burge2010,webster2015}, and momentum in expectations \citet{coibion2015} as well as cognitive biases such as confirmation bias \citet{nickerson1998confirmation} and belief perseverance \citet{anderson1980perseverance}, as natural consequences of epistemic inertia operating in different parameter regimes. These effects, absent in first-order, purely dissipative, treatments \citet{parr2022active}, emerge from second-order dynamics and yield testable predictions (such as precision-scaled relaxation times, resonance frequencies, and stopping ``distances'') that distinguish our framework from purely dissipative models.


Our approach thereby opens powerful mathematical tools traditionally relegated to physics such as symplectic geometry \citet{arnold1989mathematical}, perturbation theory \citet{holmes2012introduction}, Noether's theorem \citet{olver1993applications}, renormalization group methods \citet{wilson1975renormalization,goldenfeld1992lectures}, topological phenomena \citet{nakahara2003geometry,bernevig2013topological}, and critical point analyses \citet{strogatz2015nonlinear,sornette2006critical} for understanding cognitive and social dynamics. By recognizing beliefs as dynamical and inertial quantities, we bridge information geometry, cognitive science, and collective behavior within a unified Hamiltonian framework.

\section{Mathematical Framework}

\subsection{Beliefs as Points on Statistical Manifolds}

We model beliefs as probability distributions $q(\theta)$ parameterized by $\theta \in \mathbb{R}^n$ on a statistical manifold $\mathcal{M}$. 

For the remainder of this article we shall consider multi-variate Gaussian (MVG) beliefs and priors for convenience.


\begin{align}
q &= \mathcal{N}(\mu_q, \Sigma_q) \\
p &= \mathcal{N}(\mu_p, \Sigma_p)
\end{align}

where $\mu_\nu$ represents the believed value and $\Sigma_\nu$ represents uncertainty.

The Kullback-Leibler (KL) divergence measures the epistemic distance between an agent's belief $q$ and their prior model $p$

\begin{equation}
\text{KL}(q \| p) = \int q(x) \log \frac{q(x)}{p(x)} dx
\end{equation}



\subsection{Multi-Agent Belief Geometry}

We extend our single-agent framework to networks of interacting cognitive agents via attention. The geometric setting is a principal $G$-bundle $\pi: P \to \mathcal{C}$ where the base manifold $\mathcal{C}$ encodes agent positions and social network topology, and the gauge group $G = \mathrm{SO}(d)$ consists of rotations in belief space (see Appendix~\ref{app:gauge} for the complete geometric treatment). Each agent $i$ maintains beliefs $q_i = \mathcal{N}(\mu_i, \Sigma_i)$ anchored to a prior $p_i = \mathcal{N}(\bar{\mu}_i, \bar{\Sigma}_i)$, along with an internal reference frame $\phi_i \in \mathfrak{so}(d)$ (the Lie algebra of $\mathrm{SO}(d)$) that determines how they interpret information.

Importantly, agents cannot directly compare beliefs---they inhabit different fibers of the bundle. Instead, they must first align their gauge frames via \textbf{parallel transport operators}:

\begin{equation}
\Omega_{ij} = e^{\phi_i}e^{-\phi_j} \in \mathrm{SO}(d)
\end{equation}

 

This operator transforms agent $j$'s beliefs into agent $i$'s gauge frame of reference. This gauge structure formalizes the fundamental psychological reality that agents cannot directly share beliefs but must translate them through their respective internal interpretive perspectives. Importantly, flat gauge reproduces standard consensus models.

This operator acts by right action as

\begin{equation}
q_j \to \Omega_{ij} \cdot q_j = \mathcal{N}(\Omega_{ij}\mu_j, \Omega_{ij}\Sigma_j\Omega_{ij}^T)
\end{equation} 

For concreteness, consider $d = 3$ so that $\Omega_{ij} \in \mathrm{SO}(3)$ represents rotations; then $\phi_i \in \mathfrak{so}(3)$ is an antisymmetric matrix encoding the agent's ``interpretive orientation.''

The transformed belief can then be compared with agent $i$'s own beliefs via KL divergence

\begin{equation}
D_{ij} = D_{\mathrm{KL}}(q_i \| \Omega_{ij} \cdot q_j)
\end{equation}

Notice that this transport is, in general, asymmetric.

\subsection{Multi-Agent Free Energy}

The total variational free energy for a network of agents balances individual belief maintenance with social consensus pressure (see Appendix~\ref{app:hamiltonian} for the complete derivation and Appendix~\ref{app:social_coupling} for the generative model foundation):

\begin{align}
\mathcal{F}[\{q_i\}, \{\phi_i\}] &= \sum_i \underbrace{D_{\mathrm{KL}}(q_i \| p_i)}_{\text{Prior beliefs}} + \sum_{i,j} \underbrace{\beta_{ij} D_{\mathrm{KL}}(q_i \| \Omega_{ij} \cdot q_j)}_{\text{Social alignment}} \\
&\quad - \sum_i \underbrace{\mathbb{E}_{q_i}[\log p(o_i \mid \mu_i)]}_{\text{Sensory evidence}}
\end{align}

where $\beta_{ij}$ represents the attention agent $i$ places in agent $j$'s beliefs and we take $p_i$ to be quasi-static. The attention naturally emerges as

\begin{equation}
\beta_{ij} = \frac{\exp(-D_{\mathrm{KL}}(q_i \| \Omega_{ij} \cdot q_j)/\tau)}{\sum_k \exp(-D_{\mathrm{KL}}(q_i \| \Omega_{ik} \cdot q_k)/\tau)}
\end{equation}

with temperature $\tau$ controlling selectivity. This softmax form is not arbitrary but emerges uniquely from maximum entropy principles (Appendix~\ref{app:softmax}), and recovers the attention mechanism used in transformer architectures. The forward KL direction in the social term is similarly principled (Appendix~\ref{app:forward_kl}).

%==============================================================================
\subsection{Proper Time as Information-Theoretic Arc Length}
\label{sec:proper_time}
%==============================================================================

A fundamental issue in any dynamical theory of belief is the definition of time. Wall-clock time is unsuitable because cognitive processes operate on different timescales for different agents and contexts. We propose defining \textbf{proper time} as the information-theoretic arc length traversed on the statistical manifold---time as ``a difference which makes a difference.''

For an infinitesimal belief change $d\mu$, the proper time increment is:
\begin{equation}
\boxed{d\tau = \sqrt{d\mu^T \Sigma^{-1} d\mu} = \|d\mu\|_{\Sigma^{-1}}}
\end{equation}

This definition has several appealing properties:

\paragraph{Scale dependence.} For a high-precision agent ($\Sigma$ small, $\Sigma^{-1}$ large), a small change in $\mu$ corresponds to a large proper time---``time moves fast'' in the sense that each update is cognitively significant. For a low-precision agent, the same parametric change corresponds to a small proper time---updates are relatively insignificant. A single bit of information is enormous for a simple agent but imperceptible for a complex one.

\paragraph{Information-theoretic interpretation.} To second order, both KL directions give the same result:
\begin{equation}
\text{KL}(q + dq \| q) \approx \text{KL}(q \| q + dq) \approx \frac{1}{2} d\mu^T \Sigma^{-1} d\mu = \frac{1}{2} d\tau^2
\end{equation}
Thus proper time measures accumulated ``surprise'' or information change. The choice between KL directions is immaterial at this order.

\paragraph{Invariance.} Proper time is invariant under reparameterization of the belief space, depending only on the intrinsic geometry of the statistical manifold.

For a trajectory $\mu(t)$ parameterized by some external parameter $t$, the total proper time elapsed is:
\begin{equation}
\tau = \int \sqrt{\dot{\mu}^T \Sigma^{-1} \dot{\mu}} \, dt
\end{equation}

This is analogous to proper time in special relativity, where different observers (agents) experience time differently depending on their state. Here, the ``velocity through belief space'' determines how quickly proper time accumulates, with high-precision agents experiencing ``faster'' proper time for identical parametric motion.

%==============================================================================
\subsection{Hamiltonian Formulation of Belief Dynamics}
\label{sec:hamiltonian}
%==============================================================================

The proper time metric introduced above equips belief space with a natural notion of kinetic energy: $T = \frac{1}{2}\dot{\mu}^T \mathbf{M} \dot{\mu}$. Combined with the free energy as potential, this suggests a Hamiltonian formulation where precision plays the role of inertial mass. We adopt this as an ansatz—not derived from first principles, but motivated by the geometric structure and its empirical consequences.

%------------------------------------------------------------------------------
\subsection{The Adiabatic Approximation}
%------------------------------------------------------------------------------

Cognitive systems operate across multiple timescales often hierarchically. Beliefs generally update rapidly in response to sensory evidence when compared to priors which encode stable world-views, personality traits, or cultural assumptions.  These evolve slowly through learning and interaction. We formalize this separation via the adiabatic approximation

Let the prior parameters $(\bar{\mu}_i, \bar{\Sigma}_i)$ evolve on a slow timescale $T$, while beliefs $(\mu_i, \Sigma_i)$ evolve on a fast timescale $t$, with $\epsilon = t/T \ll 1$. 


In the quasi-static limit $\epsilon \to 0$, priors $(\bar{\mu}_i, \bar{\Sigma}_i)$ are treated as fixed external parameters, only beliefs $(\mu_i, \Sigma_i)$ are dynamical variables, and the configuration space reduces to $\mathcal{Q} = \prod_i [\mathbb{R}^d \times \mathrm{SPD}(d)]$. This approximation captures the phenomenology of rapid belief inference against a stable anchor of learned expectations and behaviors. The slow drift of priors toward equilibrated beliefs constitutes learning.

%------------------------------------------------------------------------------
\subsection{State Space and Phase Space}
%------------------------------------------------------------------------------

Each agent $i$ maintains a Gaussian belief $q_i = \mathcal{N}(\mu_i, \Sigma_i)$ anchored to a fixed prior $p_i = \mathcal{N}(\bar{\mu}_i, \bar{\Sigma}_i)$. The dynamical state vector is then

\begin{equation}
\xi_i = (\mu_i, \Sigma_i) \in \mathbb{R}^d \times \mathrm{SPD}(d)
\end{equation}

with dimension $d + \frac{d(d+1)}{2} = \frac{d(d+3)}{2}$ per agent.

The full system for $N$ agents is $\xi = (\xi_1, \ldots, \xi_N)$, living on the product manifold

\begin{equation}
\mathcal{Q} = \prod_{i=1}^N \left[\mathbb{R}^d \times \mathrm{SPD}(d)\right]
\end{equation}

To formulate Hamiltonian mechanics, we introduce \textbf{conjugate momenta}

\begin{align}
\pi_i^\mu &\in \mathbb{R}^d & &\text{(momentum conjugate to mean)} \\
\Pi_i^\Sigma &\in \mathrm{Sym}(d) & &\text{(momentum conjugate to covariance)}
\end{align}

The phase space is then the cotangent bundle $T^*\mathcal{Q}$ where $(\xi, \pi) = (\mu, \Sigma, \pi^\mu, \Pi^\Sigma)$.


%------------------------------------------------------------------------------
\subsection{Mass as Fisher Information: The Complete Derivation}
%------------------------------------------------------------------------------

The central result enabling Hamiltonian mechanics on belief space is that the effective cognitive inertia — the resistance to belief change — emerges as the total Fisher information from all sources of constraint. We derive this explicitly from the variational free energy functional.

\subsubsection{The Variational Free Energy}

The complete variational free energy for a multi-agent system decomposes as

\begin{equation}
F = \underbrace{\sum_i D_{\mathrm{KL}}(q_i \| p_i)}_{\text{complexity}} - \underbrace{\sum_i \mathbb{E}_{q_i}[\log p(o_i|c_i)]}_{\text{accuracy}} + \underbrace{\sum_{i,k} \beta_{ik} D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k])}_{\text{consensus}}
\end{equation}

where $q_i$ is agent $i$'s posterior belief, $p_i$ is agent $i$'s prior, $p(o_i|c_i)$ is the observation likelihood with hidden state $c_i$, $\Omega_{ik}[q_k]$ is neighbor $k$'s belief transported into agent $i$'s reference frame, and $\beta_{ik}$ is the attention-weighted coupling strength. The mass matrix is the Hessian of this free energy:
\begin{equation}
\mathbf{M} = \frac{\partial^2 F}{\partial\xi\partial\xi^\top}
\end{equation}

This Hessian defines the effective mass matrix $\mathbf{M}$ for belief dynamics. Unlike the intrinsic Fisher-Rao metric (which depends only on the belief geometry), this Hessian mass matrix incorporates contributions from priors, observations, and social coupling. Each term in $F$ contributes independently to the total mass.

\subsubsection{Contribution 1: Prior Precision}

The complexity cost $D_{\mathrm{KL}}(q_i \| p_i)$ penalizes deviation from the prior. Its Hessian with respect to the mean yields:
\begin{equation}
\frac{\partial^2}{\partial\mu_i\partial\mu_i^\top} D_{\mathrm{KL}}(q_i \| p_i) = \bar{\Sigma}_{p_i}^{-1} \equiv \bar{\Lambda}_{p_i}
\end{equation}

This is the prior precision, i.e. resistance to deviating from innate or learned expectations.

\subsubsection{Contribution 2: Observation Precision}

The accuracy term $-\mathbb{E}_{q_i}[\log p(o_i|c_i)]$ rewards explaining observations. For a Gaussian observation model $p(o_i|\mu_i) = \mathcal{N}(o_i \,|\, c_i, R_i)$ where $R_i$ is the sensory noise covariance:

\begin{equation}
\frac{\partial^2}{\partial\mu_i\partial\mu_i^\top} \left[-\mathbb{E}_{q_i}[\log p(o_i|c_i)]\right] = R_i^{-1} \equiv \Lambda_{o_i}
\end{equation}

This is the observation precision—the inverse sensory noise covariance. Precise observations (small $R_i$, large $\Lambda_{o_i}$) provide strong grounding that resists belief change.


\subsubsection{Contribution 3: Social Precision}

The consensus term $\sum_k \beta_{ik} D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k])$ penalizes disagreement with neighbors. Taking the Hessian:
\begin{equation}
\frac{\partial^2}{\partial\mu_i\partial\mu_i^\top} \sum_k \beta_{ik} D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k]) = \sum_k \beta_{ik} \Omega_{ik} \Sigma_{q_k}^{-1} \Omega_{ik}^\top = \sum_k \beta_{ik} \tilde{\Lambda}_{q_k}
\end{equation}

where $\tilde{\Lambda}_{q_k} = \Omega_{ik}\Lambda_{q_k}\Omega_{ik}^\top$ is the precision of neighbor $k$ transported into agent $i$'s frame.

Additionally, agent $i$ appears in the consensus terms of its neighbors $j$, contributing a \textit{reciprocal} mass:
\begin{equation}
\frac{\partial^2}{\partial\mu_i\partial\mu_i^\top} \sum_j \beta_{ji} D_{\mathrm{KL}}(q_j \| \Omega_{ji}[q_i]) = \sum_j \beta_{ji} \Lambda_{q_i}
\end{equation}

\subsubsection{The Complete Mass Formula}

Combining all contributions, the effective mass of agent $i$ is:

\begin{equation}
\boxed{
M_i = \underbrace{\bar{\Lambda}_{p_i}}_{\substack{\text{prior} \\ \text{precision}}} + \underbrace{\Lambda_{o_i}}_{\substack{\text{observation} \\ \text{precision}}} + \underbrace{\sum_k \beta_{ik}\tilde{\Lambda}_{q_k}}_{\substack{\text{incoming} \\ \text{social precision}}} + \underbrace{\sum_j \beta_{ji}\Lambda_{q_i}}_{\substack{\text{outgoing} \\ \text{social precision}}}
}
\end{equation}

This four-part structure has transparent physical meaning. The term $\bar{\Lambda}_{p_i}$ represents \textbf{prior inertia}, the resistance arising from the cost of deviating from deep expectations. The term $\Lambda_{o_i}$ represents \textbf{sensory inertia}, grounding through observation whereby precise senses anchor beliefs. The term $\sum_k \beta_{ik}\tilde{\Lambda}_{q_k}$ represents \textbf{incoming social inertia}, the effect of being pulled toward confident neighbors. Finally, the term $\sum_j \beta_{ji}\Lambda_{q_i}$ represents \textbf{outgoing social inertia}, the recoil from exerting influence on others.

\subsubsection{Physical Interpretation}

The identification of mass with total Fisher information yields several insights:

\paragraph{Sensory anchoring.} Agents with precise observations ($\Lambda_o$ large) have greater belief inertia. This seems counterintuitive; shouldn't better data make beliefs more flexible? The resolution is that precise observations provide strong evidence for the current state. An agent with low-noise sensors has high Fisher information, meaning small belief changes would dramatically worsen the likelihood fit. The agent is anchored by its own sensory precision. This predicts that experts with reliable instrumentation become harder to move than novices relying on noisy signals---not because the expert is stubborn, but because their high-fidelity observations geometrically constrain the belief manifold. The very precision that makes expertise valuable simultaneously makes it rigid.

 

\paragraph{Social amplification.} The social terms show that inertia is \textit{collective}. An agent coupled to confident neighbors inherits their precision as mass via the incoming term $\sum_k \beta_{ik}\tilde{\Lambda}_{q_k}$. A population of high-precision agents becomes collectively rigid, while uncertain agents readily reach consensus. This predicts that expertise clusters resist external perturbation: a group of confident specialists, each attending to the others, forms a mutually reinforcing mass that deflects outside information. Conversely, low-confidence agents coalesce readily around any confident neighbor, explaining how charismatic leaders can rapidly consolidate uncertain populations. The dynamics are asymmetric: confident clusters repel, uncertain populations attract.

 

\paragraph{Reciprocal costs.} The outgoing term $\sum_j \beta_{ji}\Lambda_{q_i}$ reveals that \textit{influencing others costs flexibility}. An agent that strongly affects its neighbors accumulates mass from those interactions, becoming less responsive itself. Influence is not free but instead is paid for in epistemic rigidity. The more others attend to your beliefs, the more those beliefs resist change. This has profound implications: leaders become trapped by their followers, gurus ossify under the weight of their disciples' attention, and public figures grow deaf to feedback as their audience grows. Henry Adams observed that "power is poison. Its effect on Presidents has always been tragic" \citet{adams1918education}. Our framework provides a geometric mechanism for this tragedy. The very act of projecting influence onto others accumulates as inertial mass, progressively consuming the capacity for empathy and update. The powerful become rigid not through moral failure but through the geometry of attention: when many minds attend to yours, the Fisher information contributed by those outgoing connections makes belief change increasingly costly. This predicts that influence hierarchies naturally produce epistemic stratification, with those at the top most resistant to information from below.

\begin{tcolorbox}[colback=yellow!5,colframe=orange!75,title=Important Caveat: The Hamiltonian as Ansatz]
The preceding development identifies the Fisher information metric with an inertial mass tensor. This identification is natural and geometrically principled---the Fisher metric is the unique (up to scaling) Riemannian metric on statistical manifolds. However, we emphasize that the \textbf{second-order (Hamiltonian) dynamics are an ansatz, not a derivation}.

The Fisher metric provides \emph{geometry}---a notion of distance and curvature on belief space. It does not by itself imply that beliefs evolve according to Hamiltonian mechanics with this metric as mass. We adopt this ansatz because it naturally identifies precision with inertial resistance to belief change; it predicts phenomena (oscillation, overshooting, resonance) observed empirically in attitude change research; it reduces to well-justified gradient flow in the overdamped limit $\gamma \to \infty$; and the derivations of classical sociological models (Section~\ref{sec:classical_limits}) rely \emph{only} on this overdamped limit, not on the full Hamiltonian structure.

The empirical adequacy of the underdamped regime remains to be established. The framework's predictive value lies partly in making this distinction precise: overdamped predictions are geometrically necessary, while underdamped predictions depend on the ansatz.
\end{tcolorbox}

\section{Results}

%==============================================================================
\subsection{Cognitive Phenomena from Belief Momentum}
\label{sec:cognitive-momentum}
%==============================================================================

The Hamiltonian formulation introduces a quantity absent from standard treatments of Bayesian belief updating: epistemic momentum. Just as physical momentum allows objects to flow past equilibrium, epistemic momentum allows beliefs to overshoot, oscillate, and resist change in ways that pure gradient descent fundamentally cannot capture. 
%------------------------------------------------------------------------------
\subsection{Defining Cognitive Momentum}
%------------------------------------------------------------------------------



\begin{definition}[Cognitive Momentum]

The cognitive momentum of agent $i$ is the product of epistemic mass and belief velocity

\begin{equation}
\boxed{\pi_i = M_i \dot{\mu}_i = \left(\bar{\Lambda}_{p_i} + \Lambda_{o_i}+ \sum_k \beta_{ik}\tilde{\Lambda}_{q_k} + \sum_j \beta_{ji}\Lambda_{q_i}\right) \dot{\mu}_i}
\end{equation}

where $\dot{\mu}_i$ is the rate of belief change.

\end{definition}

For an isolated agent with isotropic uncertainty $\Sigma_i = \sigma_i^2 I$, this simplifies to

\begin{equation}
\pi_i = \frac{1}{\sigma_i^2}\dot{\mu}_i = \Lambda_i \dot{\mu}_i
\end{equation}

 Momentum is not simply the velocity of belief. A confident agent (high $\Lambda$) moving slowly has the same momentum as an uncertain agent (low $\Lambda$) moving quickly. This asymmetry has interesting consequences for belief dynamics.

\begin{table}[ht]
\centering
\caption{Components of cognitive momentum and their psychological interpretations.}
\label{tab:momentum-components}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{@{} l l l @{}}
\toprule
\textbf{Component} & \textbf{Formula} & \textbf{Psychological Role} \\
\midrule
Bare momentum & $\bar{\Lambda}_{p_i}\dot{\mu}_i$ & Inertia from prior expectations \\
Social momentum & $\sum_k\beta_{ik}\tilde{\Lambda}_{q_k}\dot{\mu}_i$ & Inertia from social embedding \\
Recoil momentum & $\sum_j\beta_{ji}\Lambda_{q_i}\dot{\mu}_i$ & Inertia from influencing others \\
\bottomrule
\end{tabular}
\end{table}

%------------------------------------------------------------------------------
\subsection{Confirmation Bias as Momentum}
%------------------------------------------------------------------------------

Presently, research treats confirmation bias as a flaw in evidence evaluation and/or irrationality. Epistemic momentum yields an alternative perspective: confirmation bias is the natural dynamical consequence of beliefs possessing inertia and the underlying informational geometry holding a Fisher metric.

Therefore, we may predict that confident beliefs possess momentum that causes continued motion in their current direction even against mild opposing evidence. The stopping distance for a belief moving at velocity $\dot{\mu}$ against constant opposing force $f$ is then

\begin{equation}
d_{\text{stop}} = \frac{M_i \|\dot{\mu}_i\|^2}{2\|f\|}= \frac{\|\pi_i\|^2}{2M_i\|f\|}
\end{equation}


From energy conservation we have that the initial kinetic energy $\frac{1}{2}\pi^T M^{-1}\pi$ must be dissipated by the work done against force $f$ over distance $d$

\begin{equation}
\frac{1}{2}\pi^T M^{-1}\pi = f \cdot d_{\text{stop}}
\end{equation}
Solving for $d_{\text{stop}}$ gives the result.

This represents a distance in "epistemic" or "informational" space.

As an intuitive example, a person with a strong prior (high $\bar{\Lambda}_p$) who has been moving towards a conclusion/equilibrium (nonzero $\dot{\mu}$) doesn't simply stop when opposing evidence appears. Instead, they continue such that the cognitive momentum carries them beyond where the evidence alone would have lead them. Although this appears as confirmation bias, in our view it is actually belief inertia.

This then leads to a quantitative prediction: the ratio of stopping distances for high-precision ($\Lambda_H$) versus low-precision ($\Lambda_L$) agents is

\begin{equation}
\frac{d_H}{d_L} = \frac{\Lambda_H}{\Lambda_L}
\end{equation}

This implies that a person twice as confident takes twice as long to stop and overshoots twice as far as another.  In principle this can be tested by a clever experimentalist in order to falsify or validate the dynamical framework.

%------------------------------------------------------------------------------
\subsection{Belief Oscillation and Overshooting}
%------------------------------------------------------------------------------

Another prediction of our Hamiltonian epistemic dynamics is oscillation phenomena. Unlike gradient descent, which monotonically approaches equilibrium, Hamiltonian systems can overshoot, oscillate, and decay (see Figure~\ref{fig:phase_portraits})

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{belief_inertia/phase_portrait_damped.png}
        \caption{Overdamped ($\gamma > 2\sqrt{KM}$)}
        \label{fig:phase_overdamped}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{belief_inertia/phase_portrait_orbit.png}
        \caption{Underdamped ($\gamma < 2\sqrt{KM}$)}
        \label{fig:phase_underdamped}
    \end{subfigure}
    \caption{Phase portraits of belief dynamics. (a) Overdamped regime: beliefs decay monotonically to equilibrium. (b) Underdamped regime: beliefs exhibit oscillatory dynamics. Both simulations run from $t=0$ to $t=250$ over 10,000 steps.}
    \label{fig:phase_portraits}
\end{figure}



\subsubsection{The Damped Epistemic Oscillator}

By including dissipation (for example, attention deficits, fatigue, etc), the equation of motion becomes

\begin{equation}
M_i\ddot{\mu}_i + \gamma_i\dot{\mu}_i + \nabla_{\mu_i}F = 0
\end{equation}

where $\gamma_i > 0$ is a damping coefficient.  This equation, from the physics perspective, is the well-known driven and damped oscillator.

For small displacements from equilibrium $\mu^*$ we have

\begin{equation}
M_i\ddot{\delta\mu} + \gamma_i\dot{\delta\mu} + K_i\delta\mu = 0
\end{equation}

where $K_i = \nabla^2 F|_{\mu^*}$ represents the belief's "stiffness" (curvature of free energy at equilibrium, completely analogous to a spring).

Once again we arrive at a quantifiable prediction:

In the sub-critical ($\gamma_i < 2\sqrt{K_i M_i}$) regime, beliefs will oscillate around equilibrium with a frequency and decay time given by

\begin{equation}
\boxed{\omega = \sqrt{\frac{K_i}{M_i} - \frac{\gamma_i^2}{4M_i^2}} \approx \sqrt{\frac{\text{Evidence strength}}{\text{Epistemic mass}}}}
\end{equation}

\begin{equation}
\tau = \frac{2M_i}{\gamma_i}
\end{equation}


\subsubsection{Three Dynamical Regimes}

As the standard physics of oscillators show, the discriminant $\Delta = \gamma_i^2 - 4K_iM_i$ manifestly determines different behaviors. In the \textbf{over-damped} regime ($\Delta > 0$), beliefs decay to equilibrium monotonically without oscillation, resembling standard Bayesian updating in the literature. In the \textbf{critically damped} regime ($\Delta = 0$), the system exhibits the fastest approach to equilibrium without oscillation, suggesting this may be optimal for rapid learning. In the \textbf{under-damped} regime ($\Delta < 0$), beliefs oscillate around the equilibrium value, overshooting periodically before equilibrating, producing distinctly non-standard Bayesian dynamics.

As an intuitive and timely example, consider an agent (such as a conspiracy theorist) with high precision (strong prior beliefs) and low damping (resistance to evidence). When confronted with strong contradictory evidence, the agent will generally exhibit \textbf{initial resistance} as the high mass $M = \Lambda$ resists the force of evidence, followed by \textbf{acceleration} as persistent evidence eventually accelerates belief change, then \textbf{overshoot} as momentum carries belief past the truth, then \textbf{oscillation} as belief swings between acceptance and rejection, and finally \textbf{settling} as damping eventually brings convergence to equilibrium.

This pattern (resist, over-correct, oscillate) is consistent with phenomena documented in attitude change and belief correction research \citet{Eagly1993, Lewandowsky2012} but remains unexplained by standard Bayesian models. Here we find a natural and intuitive account.

%------------------------------------------------------------------------------
\subsection{Cognitive Resonance}
%------------------------------------------------------------------------------

Interestingly, general oscillatory systems exhibit resonance phenomena whereby maximum response occurs when the driving frequency matches the system's natural frequency. In our epistemic view this then has direct implications for persuasion and learning.


A prediction presents itself: periodic evidence driving achieves maximum belief change at the agents belief resonance frequency given by

\begin{equation}
\boxed{\omega_{\text{res}} = \sqrt{\frac{K_i}{M_i}} = \sqrt{\frac{\text{Evidence strength} \times \text{Precision}}{\text{Epistemic mass}}}}
\end{equation}


\subsubsection{Amplitude at Resonance}

For example, with sinusoidal forcing $f(t) = f_0\cos(\omega t)$, the steady-state amplitude is shown (in physics/engineering) to be

\begin{equation}
A(\omega) = \frac{f_0/M_i}{\sqrt{(\omega_0^2 - \omega^2)^2 + (\gamma\omega/M_i)^2}}
\end{equation}

where $\omega_0 = \sqrt{K/M}$ is the system's "natural" frequency.

At resonance ($\omega = \omega_{\text{res}} \approx \omega_0$) then, we have

\begin{equation}
A_{\text{max}} = \frac{f_0}{\gamma_i \sqrt{K_i/M_i}} = \frac{f_0}{\gamma_i}\sqrt{\frac{M_i}{K_i}}
\end{equation}

Curiously this implies that high-mass (confident) agents have larger resonance amplitudes rather than smaller. While they resist off-resonance forcing, properly timed evidence produces dramatic swings. This prediction then offers myriad applications in psychological/sociological fields (education, advertising, negotiating, therapy, etc).


%------------------------------------------------------------------------------
\subsection{Belief Perseverance}
%------------------------------------------------------------------------------

The characteristic time for a belief to relax toward equilibrium in a social setting is given by

\begin{equation}
\boxed{\tau = \frac{M_i}{\gamma_i} = \frac{\bar{\Lambda}_{p_i} + \Lambda_{o_i}+ \sum_k\beta_{ik}\tilde{\Lambda}_{q_k} + \sum_j\beta_{ji}\Lambda_{q_i}}{\gamma_i}}
\end{equation}


High-precision beliefs have long decay times. This suggests phenomena where agents tend to hold onto beliefs even after thorough debunking and evidence to their contrary.

For example, if agent A has precision $\Lambda_A = 10$ and agent B has $\Lambda_B = 1$ (both with equal damping $\gamma$), then

\begin{equation}
\frac{\tau_A}{\tau_B} = \frac{\Lambda_A}{\Lambda_B} = 10
\end{equation}

Agent A's false beliefs persist ten times longer than that of B's, despite identical evidence exposure.

\subsubsection{The Debunking Problem}

Typically debunking assumes beliefs respond instantaneously to evidence yet our theory of epistemic momentum predicts that immediate debunking is ineffective.  The belief should flow past the correction target. Furthermore, repeated debunking, if not properly timed, can lead to amplification (a well studied phenomenon in debunking studies).  A candidate method for debunking, then, is to properly time the belief trajectory before reinforcing the correction.  However, predicting that time scale for a given agent may be difficult.


%------------------------------------------------------------------------------
\subsection{Sociology and Multi-Agent Momentum Transfer}
%------------------------------------------------------------------------------

When agents interact through the attention free energy ($\beta_{ij}$ term), momentum can transfer between beliefs, i.e. one agent's beliefs affects another's. This suggests a system of coupled equations of motion given an attention pattern of a multi-agent system.

\subsubsection{Coupled Equations of Motion}

The full multi-agent dynamics with damping are

\begin{equation}
\boxed{M_i\ddot{\mu}_i + \gamma_i\dot{\mu}_i + \nabla_{\mu_i}F = 0}
\end{equation}

We may expand the gradient as 

\begin{equation}
M_i\ddot{\mu}_i = -\gamma_i\dot{\mu}_i - \bar{\Lambda}_{p_i}(\mu_i - \bar{\mu}_i) - \sum_k\beta_{ik}\tilde{\Lambda}_{q_k}(\mu_i - \tilde{\mu}_k) - \sum_j\beta_{ji}\Lambda_{q_i}\Omega_{ji}^T(\tilde{\mu}_i^{(j)} - \mu_j)
\end{equation}

Then this can be written as

\begin{equation}
\boxed{\underbrace{M_i\ddot{\mu}_i}_{\text{Inertia}} = -\underbrace{\gamma_i\dot{\mu}_i}_{\text{Damping}} - \underbrace{\nabla_{\mu_i}F_{\text{prior}}}_{\text{Prior force}} - \underbrace{\nabla_{\mu_i}F_{\text{consensus}}}_{\text{Social force}}}
\end{equation}

\subsubsection{Momentum Transfer Theorem}

\begin{theorem}[Momentum Transfer Between Agents]

When agent $k$ changes belief, it transfers epistemic momentum to agent $i$ according to

\begin{equation}
\frac{d\pi_i}{dt}\bigg|_{\text{from } k} = -\beta_{ik}\tilde{\Lambda}_{q_k}(\mu_i - \tilde{\mu}_k) - \beta_{ki}\Lambda_{q_i}\Omega_{ki}^T(\tilde{\mu}_k^{(i)} - \mu_i)
\end{equation}

The total momentum transfer over a given interaction time scale $[0, T]$ is

\begin{equation}
\boxed{\Delta\pi_i = -\int_0^T \left[\beta_{ik}\tilde{\Lambda}_{q_k}(\mu_i - \tilde{\mu}_k) + \beta_{ki}\Lambda_{q_i}\Omega_{ki}^T(\tilde{\mu}_k^{(i)} - \mu_i)\right] dt}
\end{equation}
\end{theorem}

\subsubsection{Conservation and Non-Conservation}

Without priors and damping, the total momentum is a conserved quantity.

\begin{equation}
\frac{d}{dt}\sum_i \pi_i = 0 \quad \text{(closed system)}
\end{equation}

In contrast,  with priors and damping, momentum is assuredly not conserved. Momentum flows into the environment (the prior) and is then dissipated

\begin{equation}
\frac{d}{dt}\sum_i \pi_i = -\sum_i\gamma_i\dot{\mu}_i - \sum_i\bar{\Lambda}_{p_i}(\mu_i - \bar{\mu}_i)
\end{equation}


This allows us to define a momentum current from agent $k$ to agent $i$ as

\begin{equation}
J_{k\rightarrow i} = \beta_{ik}\tilde{\Lambda}_{q_k}(\tilde{\mu}_k - \mu_i)
\end{equation}

This satisfies the continuity equation

\begin{equation}
\dot{\pi}_i + \gamma_i\dot{\mu}_i + \bar{\Lambda}_{p_i}(\mu_i - \bar{\mu}_i) = \sum_k J_{k\rightarrow i}
\end{equation}

We find that momentum flows from agents with different beliefs via attention $\beta_{ik}$ and sender precision $\Lambda_{q_k}$. High-precision agents are powerful momentum sources as their motion strongly affects coupled neighbors. However, their strength is weighted by their relative attentions $\beta_{ij}$

%------------------------------------------------------------------------------
\subsection{Summary}
%------------------------------------------------------------------------------

\begin{table}[ht]
\centering
\caption{Testable predictions from cognitive momentum theory.}
\label{tab:predictions}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{@{} p{3cm} p{5cm} p{5cm} @{}}
\toprule
\textbf{Phenomenon} & \textbf{Prediction} & \textbf{Experimental Test} \\
\midrule
Confirmation bias & Stopping distance is $\propto$ precision & Measure belief change latency vs. covariance \\[6pt]
Belief oscillation & Under-damped agents overshoot truth and oscillate & Track belief trajectories over time \\[6pt]
Resonance & Optimal persuasion occurs at $\omega_{\text{res}} = \sqrt{K/M}$ & Vary message timing, measure change \\[6pt]
Perseverance & Decay time $\tau = M/\gamma$ & Measure false belief persistence vs. uncertainty \\[6pt]
Social momentum & High-$\Lambda$ agents transfer more momentum & Attention vs. source confidence \\[6pt]
Recoil & Persuaders become harder to persuade & Measure attitude stiffness after persuasion attempts \\
\bottomrule
\end{tabular}
\end{table}

Our epistemic momentum framework unifies seemingly disparate phenomena such as confirmation bias, belief perseverance, oscillation, and social influence into manifestations of a single underlying epistemic Hamiltonian mechanics. Beliefs are not just updated,  they are accelerated. Evidence does not instantly change minds but rather applies an epistemic force. Finally, confident beliefs don't only resist change rather, they possess epistemic inertia that carries them further than evidence alone would have lead them.

%==============================================================================
\section{Classical Sociological Models as Limiting Cases}
\label{sec:classical_limits}
%==============================================================================

We now demonstrate that several classical models from sociology and network science emerge from the overdamped limit of our framework. These derivations do \emph{not} depend on the inertial ansatz---they follow rigorously from gradient flow on the statistical manifold.

\subsection{DeGroot Social Learning}

\subsubsection{Classical Formulation}

DeGroot's model (1974) describes social learning as iterative averaging of neighbors' beliefs:
\begin{equation}
\mu_i(t+1) = \sum_j w_{ij} \mu_j(t)
\label{eq:degroot_classical}
\end{equation}
where $W = [w_{ij}]$ is a row-stochastic matrix ($\sum_j w_{ij} = 1$) representing social influence weights. Under mild conditions, beliefs converge to a consensus determined by the network structure.

\subsubsection{Sociological Context}

DeGroot's model captures the fundamental sociological insight that individuals update beliefs by averaging opinions from their social network. It has been applied to jury deliberation, scientific consensus formation, and organizational decision-making. However, the model treats influence weights $w_{ij}$ as exogenous and fixed, providing no mechanism for how attention emerges from belief similarity.

\subsubsection{Derivation from VFE Framework}

\begin{proposition}[DeGroot as VFE Limit]
The DeGroot update rule \eqref{eq:degroot_classical} emerges from gradient flow on the VFE under:
\begin{enumerate}[label=(\roman*)]
\item Overdamped dynamics: $\gamma \to \infty$
\item Low uncertainty: $\Sigma_i \to \sigma^2 I$ with $\sigma^2$ small
\item Flat manifold: $\Omega_{ij} = I$ (shared reference frames)
\item No self-coupling: $\alpha = 0$
\item No observations: $\lambda_{\text{obs}} = 0$
\item Fixed attention: $\beta_{ij} = w_{ij}$ (constant, not softmax)
\end{enumerate}
\end{proposition}

\begin{proof}
\textbf{Step 1: Simplify VFE.}
Under these conditions, the free energy reduces to:
\begin{equation}
F[\mu] = \frac{\lambda_\beta}{2\sigma^2} \sum_{i,j} w_{ij} \|\mu_i - \mu_j\|^2
\end{equation}

\textbf{Step 2: Compute gradient.}
\begin{align}
\nabla_{\mu_i} F &= \frac{\lambda_\beta}{\sigma^2} \sum_j w_{ij} (\mu_i - \mu_j) \\
&= \frac{\lambda_\beta}{\sigma^2} \left[\mu_i - \sum_j w_{ij} \mu_j\right] \quad \text{(using row-stochasticity)}
\end{align}

\textbf{Step 3: Apply gradient flow.}
The mass matrix is $M_i \approx \sigma^{-2} I$. Gradient flow gives:
\begin{equation}
\frac{d\mu_i}{d\tau} = -M_i^{-1}\nabla_{\mu_i} F = -\lambda_\beta\left(\mu_i - \sum_j w_{ij}\mu_j\right)
\end{equation}

\textbf{Step 4: Discretize.}
Forward Euler with $\Delta\tau = 1/\lambda_\beta$:
\begin{equation}
\mu_i(\tau + \Delta\tau) = \sum_j w_{ij}\mu_j(\tau)
\end{equation}

This is exactly the DeGroot update \eqref{eq:degroot_classical}. \qed
\end{proof}

\subsubsection{What the Unified Framework Adds}

\paragraph{Dynamic attention.} Removing the fixed-attention assumption and using softmax attention, influence weights become endogenous:
\begin{equation}
\beta_{ij}(\tau) = \frac{\exp(-\|\mu_i(\tau) - \mu_j(\tau)\|^2 / (2\sigma^2\kappa))}{\sum_k \exp(-\|\mu_i(\tau) - \mu_k(\tau)\|^2 / (2\sigma^2\kappa))}
\end{equation}
Agents pay more attention to similar others, creating homophily as an emergent property rather than assumption.

\paragraph{Uncertainty dynamics.} Relaxing the uniform-$\Sigma$ assumption, beliefs become full distributions $q_i = \N(\mu_i, \Sigma_i)$. Uncertainty can increase or decrease over time, capturing phenomena like pluralistic ignorance or confidence polarization that mean-only models miss.

\paragraph{Epistemic inertia.} When agents receive asymmetric attention (some have many followers), the outgoing social mass term becomes significant. High-attention agents develop higher mass, making their beliefs more resistant to change---a mechanistic explanation for rigidity in positions of authority.

\subsection{Friedkin-Johnsen Opinion Dynamics}

\subsubsection{Classical Formulation}

Friedkin and Johnsen (1990) extended DeGroot by introducing ``stubbornness''---attachment to initial opinions:
\begin{equation}
\mu_i^* = \alpha_i \mu_i(0) + (1 - \alpha_i) \sum_j w_{ij} \mu_j^*
\label{eq:fj_classical}
\end{equation}
where $\alpha_i \in [0,1]$ represents agent $i$'s resistance to social influence and $\mu^*$ denotes equilibrium. This model better captures polarization and persistent disagreement, as stubborn agents prevent full consensus.

\subsubsection{Sociological Context}

The Friedkin-Johnsen model addresses a key empirical puzzle: social groups often fail to reach consensus despite dense communication. The stubbornness parameter $\alpha_i$ is typically interpreted as a personality trait or ideological commitment. However, this raises the question: what determines stubbornness, and can it change over time?

\subsubsection{Derivation from VFE Framework}

\begin{proposition}[Friedkin-Johnsen as VFE Equilibrium]
The Friedkin-Johnsen equilibrium emerges from the VFE framework under DeGroot conditions plus:
\begin{enumerate}[label=(\roman*), start=7]
\item Non-zero self-coupling: $\alpha > 0$
\item Fixed priors: $p_i = \N(\mu_i(0), \Sigma_p)$ (initial beliefs)
\end{enumerate}
Moreover, the stubbornness parameter $\alpha_i$ emerges from prior precision and social context.
\end{proposition}

\begin{proof}
\textbf{Step 1: VFE with self-coupling.}
Including the prior term:
\begin{equation}
F[\mu] = \frac{\alpha}{2\Sigma_p} \sum_i \|\mu_i - \mu_i(0)\|^2 + \frac{\lambda_\beta}{2\sigma^2} \sum_{i,j} w_{ij} \|\mu_i - \mu_j\|^2
\end{equation}

\textbf{Step 2: Equilibrium condition.}
At steady state, $\nabla_{\mu_i} F = 0$:
\begin{equation}
\frac{\alpha}{\Sigma_p}(\mu_i^* - \mu_i(0)) + \frac{\lambda_\beta}{\sigma^2}\left(\mu_i^* - \sum_j w_{ij}\mu_j^*\right) = 0
\end{equation}

\textbf{Step 3: Solve for equilibrium.}
\begin{equation}
\mu_i^* = \frac{\alpha/\Sigma_p}{\alpha/\Sigma_p + \lambda_\beta/\sigma^2}\mu_i(0) + \frac{\lambda_\beta/\sigma^2}{\alpha/\Sigma_p + \lambda_\beta/\sigma^2}\sum_j w_{ij}\mu_j^*
\end{equation}

Define the \emph{emergent stubbornness}:
\begin{equation}
\alpha_i' = \frac{\alpha/\Sigma_p}{\alpha/\Sigma_p + \lambda_\beta/\sigma^2}
\label{eq:emergent_stubbornness}
\end{equation}

Then $\mu_i^* = \alpha_i' \mu_i(0) + (1-\alpha_i')\sum_j w_{ij}\mu_j^*$, matching \eqref{eq:fj_classical}. \qed
\end{proof}

\subsubsection{Mechanistic Stubbornness}

The key sociological insight is that stubbornness $\alpha_i'$ in \eqref{eq:emergent_stubbornness} is \emph{not} a fixed personality trait but emerges from two sources:

\paragraph{Prior precision $\Sigma_p^{-1}$.} Agents with strong initial convictions (low $\Sigma_p$) exhibit high stubbornness. This captures ideological commitment or expertise: a climate scientist has high prior precision about global warming, making them resistant to contrarian social influence.

\paragraph{Social coupling strength $\lambda_\beta \sum_j w_{ij}$.} Agents experiencing intense social pressure (many influential neighbors) become \emph{less} stubborn in equilibrium. However, this same pressure increases their \emph{inertial mass}, slowing their rate of approach to equilibrium---a subtle but important distinction.

The framework predicts that the same individual should exhibit different degrees of stubbornness across different social contexts---contradicting trait-based theories that treat resistance to influence as a stable personality characteristic.

\subsection{Echo Chambers and Polarization}

\subsubsection{Phenomenon}

Echo chambers constitute a self-reinforcing dynamical process: individuals preferentially attend to similar others (homophily), causing in-group beliefs to converge while out-group beliefs diverge, culminating in isolation as cross-group communication declines.

\subsubsection{Derivation from VFE Framework}

\begin{proposition}[Emergent Homophily and Polarization]
Softmax attention automatically creates homophilic coupling. When initial belief distributions are multimodal, this leads to stable polarized states with within-group consensus and cross-group divergence.
\end{proposition}

\begin{proof}[Proof Sketch]
\textbf{Step 1: Softmax creates homophily.}
For Gaussian beliefs with common covariance:
\begin{equation}
\beta_{ij} = \frac{\exp(-\|\mu_i - \mu_j\|^2 / (2\sigma^2 \kappa))}{\sum_k \exp(-\|\mu_i - \mu_k\|^2 / (2\sigma^2 \kappa))}
\end{equation}
Similar beliefs (small $\|\mu_i - \mu_j\|$) yield high $\beta_{ij}$; dissimilar beliefs yield low $\beta_{ij}$.

\textbf{Step 2: Positive feedback loop.}
The gradient flow is $d\mu_i/d\tau \propto \sum_j \beta_{ij}(\mu_j - \mu_i)$.

This creates positive feedback: similar beliefs $\to$ high attention $\to$ further convergence $\to$ higher attention $\to$ cross-group attention vanishes.

\textbf{Step 3: Stability condition.}
For two groups $A$ and $B$, the polarized state is stable when:
\begin{equation}
\|\mu_A - \mu_B\|^2 > 2\sigma^2 \kappa \log N
\label{eq:polarization_threshold}
\end{equation}
where $N$ is the number of agents. \qed
\end{proof}

\subsubsection{Phase Transition in Polarization}

The stability condition \eqref{eq:polarization_threshold} reveals a \emph{phase transition} in the temperature parameter $\kappa$:

\paragraph{High temperature ($\kappa$ large):} Attention is diffuse, cross-group communication persists, system converges to global consensus.

\paragraph{Low temperature ($\kappa$ small):} Attention is sharp, cross-group communication collapses, system locks into polarized state.

The critical temperature scales with initial belief separation:
\begin{equation}
\kappa^{\text{crit}} \sim \frac{\|\mu_A(0) - \mu_B(0)\|^2}{2\sigma^2 \log N}
\end{equation}

\subsubsection{Connection to Filter Bubbles}

Social media platforms that use engagement-based ranking effectively lower $\kappa$ (sharpen attention toward similar content). The VFE framework predicts this design choice should increase polarization, consistent with empirical observations. Interventions to reduce polarization should target $\kappa$: increasing exposure diversity (raising temperature) or increasing epistemic humility (raising uncertainty $\sigma^2$) can prevent the polarization phase transition.

\subsection{Bounded Confidence Models}

\subsubsection{Classical Formulation}

Hegselmann-Krause (2002) and Deffuant et al. (2000) introduced bounded confidence: agents only interact with others within a threshold distance $\epsilon$:
\begin{equation}
\mu_i(t+1) = \frac{1}{|N_i(\epsilon)|}\sum_{j \in N_i(\epsilon)} \mu_j(t)
\end{equation}
where $N_i(\epsilon) = \{j : |\mu_j - \mu_i| < \epsilon\}$.

\subsubsection{Correspondence with VFE Framework}

\begin{proposition}[Bounded Confidence as Low-Temperature Limit]
The bounded confidence dynamics approximate the VFE framework in the low-temperature regime $\kappa \to 0$, with effective threshold:
\begin{equation}
\epsilon_{\text{eff}} \approx \sigma\sqrt{2\kappa \log N}
\end{equation}
\end{proposition}

As $\kappa \to 0$, the softmax becomes increasingly sharp. Agents within the effective radius receive substantial attention; those outside receive exponentially suppressed attention.

\subsubsection{Key Difference: Soft vs. Hard Threshold}

The VFE framework produces a \emph{soft} threshold (smooth exponential decay) rather than the classical hard cutoff. This is more psychologically realistic---people don't entirely ignore slightly-too-distant opinions but attend to them with diminishing weight. The soft threshold is also mathematically tractable (differentiable everywhere) and parameterized by $\kappa$, allowing interpolation between regimes.

\subsubsection{Adaptive Threshold}

Unlike fixed-$\epsilon$ models, the effective threshold depends dynamically on parameters: $\epsilon_{\text{eff}} = f(\sigma, \kappa, N)$. Higher epistemic uncertainty $\sigma$ increases tolerance for distant opinions, consistent with findings that epistemic humility reduces polarization.

\subsection{Confirmation Bias from Epistemic Mass}

Even in the overdamped regime, the mass matrix affects dynamics.

\begin{proposition}[Confirmation Bias from Epistemic Mass]
Agents with high prior precision or many followers update more slowly in response to the same evidence gradient, without requiring any non-Bayesian mechanisms.
\end{proposition}

\begin{proof}[Proof Sketch]
The gradient flow dynamics are:
\begin{equation}
\frac{d\mu_i}{d\tau} = -M_i^{-1} \nabla_{\mu_i} F
\end{equation}

Update magnitude:
\begin{equation}
\|d\mu_i\| \propto M_i^{-1} \|\nabla_{\mu_i} F\|
\end{equation}

Agents with large $M_i$ (high precision, many followers) update more slowly for the same gradient. \qed
\end{proof}

\paragraph{The outgoing attention term.} The term $\sum_j \beta_{ji}\Lambda_{q,i}$ represents ``mass from being attended to.'' Agents with many followers accumulate epistemic mass, becoming more resistant to change. This provides a geometric mechanism for the observation that influence and flexibility are in tension. Henry Adams observed that ``power is poison''---our framework provides a geometric mechanism for this tragedy.

\subsection{Social Impact Theory}

\subsubsection{Classical Formulation}

Latan\'e's Social Impact Theory (1981) posits that social influence is a multiplicative function of three factors:
\begin{equation}
\text{Impact} = f(\text{Strength} \times \text{Immediacy} \times \text{Number})
\end{equation}

\subsubsection{Mapping to VFE Framework}

The mass matrix provides a natural quantitative interpretation:

\paragraph{Strength $\leftrightarrow$ $\Sigma_{q,j}^{-1}$.} Source precision (confidence/expertise) contributes directly to the mass experienced by the target.

\paragraph{Immediacy $\leftrightarrow$ Transport penalty $\|\Omega_{ij} - I\|$.} Agents who are ``close'' have aligned frames ($\Omega_{ij} \approx I$), receiving high attention. Distant agents incur large KL penalties and receive low attention.

\paragraph{Number $\leftrightarrow$ $\sum_j$.} More sources yield more terms in the social mass sum.

\subsubsection{What the VFE Framework Adds}

\paragraph{Exact quantitative formula.} Latan\'e's principle is qualitative; the VFE framework gives precise predictions.

\paragraph{Time-varying impact.} As beliefs and attention evolve, mass changes dynamically.

\paragraph{Asymmetry.} Social impact is not reciprocal: $\Delta M_i^{(j)} \neq \Delta M_j^{(i)}$.

\subsubsection{Caveat}

This is an \emph{interpretive correspondence}, not a formal equivalence. The VFE framework provides one specific quantitative instantiation of Latan\'e's qualitative principle.

\subsection{Diffusion of Innovations}

\subsubsection{Classical Formulation}

Rogers (1962) identified a characteristic S-curve pattern in innovation adoption, with distinct adopter categories: innovators (2.5\%), early adopters (13.5\%), early majority (34\%), late majority (34\%), and laggards (16\%).

\subsubsection{Correspondence with VFE Framework}

\begin{proposition}[S-Curve from Attention Dynamics]
The logistic adoption curve emerges when agents decide between adopt/reject based on social attention from prior adopters, with heterogeneous prior precisions determining adoption order.
\end{proposition}

The Rogers categories emerge naturally from the distribution of epistemic mass $M_i$: \textbf{Innovators} have low mass due to uncertain priors and few anchoring connections; \textbf{early adopters} have moderate mass with network positions exposing them to innovators; the \textbf{early and late majority} have moderate-to-high mass and require substantial adopter signal; and \textbf{laggards} have the highest mass due to extreme prior certainty or isolation.

\paragraph{Commitment trap.} Early adopters who influence others accumulate social mass from those attention connections, becoming resistant to abandoning the innovation even if problems emerge. This explains why early advocates are often the last to admit failure.

\subsection{Summary of Derivations}

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Rigor} & \textbf{Depends on Ansatz?} & \textbf{Notes} \\
\midrule
DeGroot & Exact & No & Overdamped limit \\
Friedkin-Johnsen & Exact & No & Equilibrium solution \\
Echo Chambers & Derived & No & Softmax attention \\
Bounded Confidence & Approximate & No & Low-$\kappa$ limit \\
Confirmation Bias & Geometric & Partially & Mass matrix interpretation \\
Social Impact Theory & Interpretive & No & Qualitative correspondence \\
Diffusion of Innovations & Approximate & No & S-curve from heterogeneous mass \\
\bottomrule
\end{tabular}
\caption{Quality assessment of derivations. Core results depend only on gradient flow, not the inertial ansatz.}
\label{tab:rigor}
\end{table}


\section{Discussion}

\subsection{Unifying Phenomenological Models}

For decades, researchers across psychology, neuroscience, and opinion dynamics have empirically modeled belief change using spring-mass metaphors without theoretical justification. Kaplowitz and Fink's damped oscillator model of attitude change \citet{kaplowitz1983, kaplowitz1992}, which successfully predicted oscillation and overshoot in response to persuasive messages \citet{fink2002}, treated mass and damping as free parameters which they fit to data. Similarly, bounded confidence models in opinion dynamics \citet{hegselmann2002, deffuant2000}, momentum effects in economic expectations \citet{coibion2015}, and overshoot phenomena in perceptual adaptation \citet{webster2015} all invoke inertial dynamics without explaining their origin.

Our central contribution is showing that these are not mere analogies but instead consequences of variational inference on curved statistical manifolds. The Fisher information metric \textit{is} the inertial mass tensor for epistemic dynamics; damping emerges from dissipative terms in the variational principle where the restoring force is the gradient of the variational free energy. This provides a principled basis for parameter values previously treated as phenomenological: \textbf{mass} equals $\Lambda_{\text{prior}} + \Lambda_{\text{observation}} + \Lambda_{\text{social}}$, the total precision of an agent's belief; \textbf{damping} $\gamma$ arises from dissipation due to attention, metabolic costs, or environmental noise; and the \textbf{spring constant} $K$ corresponds to the curvature of the free energy landscape at equilibrium. The framework thus unifies disparate empirical observations under a single geometric principle: beliefs are points on a Riemannian manifold, and their dynamics are geodesic motion with friction.

\subsection{Empirical Validation}

We validated the framework on the helicopter task \citet{mcguire2014, nassar2010, nassar2012}, where high observation noise ($\sigma_o^2$ large) and frequent environmental change (hazard rate $H \approx 0.1$) place observers in conditions which our theory predicts should produce overdamped dynamics. High noise reduces sensory precision $\Lambda_o$, decreasing effective mass whereas environmental volatility increases optimal damping. Together, these push dynamics toward the overdamped limit where $\gamma \gg 2\sqrt{KM}$.

Indeed, the simple delta rule (the zero-momentum limit of our framework) provided adequate fit for 97\% of participants (31/32 by BIC). The fitted momentum parameter, while statistically non-zero ($\beta = 0.003 \pm 0.006$, $p = 0.01$), was practically negligible. This is not a failure of the theory but a successful prediction: the helicopter task was designed to study learning rate adaptation in volatile environments, not belief inertia. Its parameters place observers firmly in the overdamped regime where our framework reduces to standard models.

This explains why gradient descent suffices here while oscillatory dynamics appear elsewhere. Kaplowitz and Fink \citet{kaplowitz1992} observed attitude oscillation in persuasion studies where participants had strong prior commitments (high $\Lambda_{\text{prior}}$) confronted with credible counter messages (high $\Lambda_{\text{observation}}$) which represent precisely the conditions our framework predicts should produce underdamped dynamics. Our present theory provides the missing bridge -  both phenomena emerge from the same equations in different parameter regimes.

\subsection{Why Was This Overlooked?}

The connection between precision and inertial mass, despite its naturalness, has remained hidden at the intersection of several disciplines that rarely communicate. 

Psychology has historically focused on static biases and heuristics, cataloging the ways beliefs deviate from normative standards rather than the temporal dynamics of how beliefs change \citet{nickerson1998}. Researchers have been reluctant to ask "how fast does this belief evolve?" with appropriate dynamical tools. When oscillation was observed \citet{kaplowitz1983, fink2002}, it remained isolated within communication science, never connecting to the variational principles that would explain why attitudes behave like mechanical systems.

Neuroscience, meanwhile, focuses on gradient-based learning primarily because neural systems are highly damped. Synaptic time constants, metabolic constraints, and homeostatic regulation ensure that neural dynamics operate in the overdamped regime \citet{friston2008hierarchical} \citet{bogacz2017tutorial}. This has led researchers to overlook oscillatory or momentum-like behavior that might otherwise be visible. The brain appears to do gradient descent because it operates in a regime where inertial effects are suppressed rather than because momentum is absent from the underlying mathematics. It has been there the whole time.

Information geometry, meanwhile, provides the mathematical language for these ideas but was developed largely within statistics and machine learning, far from psychological or sociological theory. The Fisher metric was studied as an abstract structure on probability spaces \citet{Amari2016}, not as an inertia tensor governing dynamics.

 
Sociology has developed sophisticated models of opinion dynamics, social influence, and collective behavior \citet{degroot1974, friedkin2011, flache2017}, yet these frameworks typically assume instantaneous averaging or threshold-based contagion rather than momentum-carrying dynamics. The insight that social attention directed toward an agent accumulates as epistemic mass (thereby making the attended-to individual more resistant to belief change) requires bridging network science with variational mechanics. This connection was obscured by the disciplinary boundaries separating formal sociology from the physics-inspired methods that would reveal it.

\subsection{Cognitive Biases as Emergent Phenomena}

A striking implication of our framework is that phenomena often attributed to "cognitive biases" emerge naturally from epistemic inertia rather than requiring separate psychological mechanisms.

\textbf{Belief perseverance} is the tendency for beliefs to persist even after their evidential basis has been discredited \citet{anderson1980, ross1975} follows directly from epistemic mass. High-precision beliefs have large $M = \Lambda$ and thus long relaxation times $\tau = M/\gamma$. They resist change not due to irrationality but because mass resists acceleration. The debriefing paradigm, which demonstrates that beliefs persist after subjects learn the initial evidence was fabricated, is explained by inertia. 

\textbf{The continued influence effect}, whereby misinformation continues to affect reasoning even after correction \citet{Lewandowsky2012}, similarly reflects momentum decay rather than memory failure. Corrections apply a counter-force, but if the original misinformation was encoded with high precision such that the belief's momentum carries it past the corrected equilibrium before dissipation brings it to rest.

\textbf{Confirmation bias}, the tendency to seek and weight evidence consistent with existing beliefs \citet{nickerson1998}, can be reinterpreted as a consequence of inertial dynamics. Beliefs with high momentum in a particular direction are less deflected by contradictory evidence than by confirmatory evidence. This is not a bias in the pejorative sense but a geometrical consequence of how informationally massive objects respond to epistemic forces.

\textbf{The rigidity of influence.} Perhaps most striking is the prediction that influence itself accumulates as inertial mass. The outgoing social term $\sum_j \beta_{ji}\Lambda_{q_i}$ in the mass formula means that agents whose beliefs are attended to by many others become progressively more rigid. This provides a geometric mechanism for observations that leaders, experts, and public figures often exhibit reduced sensitivity to feedback; what Adams called "poison" and which Solzhenitsyn poetically described as "power is a poison well known for thousands of years... for those who are unaware of any higher sphere, it is a deadly poison. For them there is no antidote" \citet{adams1918education, solzhenitsyn1973gulag}. The phenomenon is not necessarily a moral failure but mathematical inevitability. When many minds attend to yours, the Fisher information contributed by those outgoing connections makes belief change increasingly costly. Empathy, the capacity to update one's model of others, requires precisely the flexibility that influence consumes. This predicts that influence hierarchies naturally produce epistemic stratification, with those at the top most resistant to information from below, and suggests that the "isolation of power" documented by historians and political scientists emerges from the same geometric principles governing individual belief dynamics.


This reframing provides a mechanistic basis for intervention. If belief perseverance stems from high precision rather than stubbornness, then interventions targeting uncertainty (increasing $\gamma/M$) may be more effective than those targeting content. Similarly, if the rigidity of leadership emerges from accumulated social attention, then institutional designs that distribute attention more evenly may preserve epistemic flexibility at the top.

\subsection{Proposed Experimental Tests}

The predictions derived above distinguish the inertial framework from standard first-order Bayesian models. We outline experimental paradigms that could test these predictions. Detailed implementation is left for future work.

\subsubsection{Belief Oscillation Under Strong Counter-Evidence}

\textbf{Prediction:} High-confidence agents should overshoot equilibrium and exhibit non-monotonic belief trajectories when confronted with strong counter-evidence.

\textbf{Design:} Measure participants' prior beliefs and confidence on contentious topics via incentivized elicitation. Present strong, credible counter-evidence and track belief trajectories via repeated measurements (e.g., slider scales at 1-minute intervals over 20 minutes). 

\textbf{Discrimination:} Standard Bayesian models predict monotonic convergence toward the posterior. The inertial framework predicts that high-confidence participants may transiently overshoot, briefly adopting positions more extreme than the evidence warrants before settling to equilibrium. Any observed non-monotonicity falsifies purely dissipative models.

\subsubsection{Precision-Dependent Relaxation Times}

\textbf{Prediction:} Belief relaxation time $\tau$ scales linearly with prior precision: $\tau = \Lambda/\gamma$.

\textbf{Design:} Measure prior confidence via betting procedures or confidence intervals. Following exposure to counter-evidence, measure time to reach stable posterior beliefs across participants varying in initial confidence.

\textbf{Discrimination:} The framework predicts that participants with twice the initial precision require twice the relaxation time, independent of the direction or magnitude of belief change. Standard models predict relaxation rates depend on evidence strength rather than prior confidence.

\subsubsection{Resonant Persuasion}

\textbf{Prediction:} Periodic messaging achieves maximum belief change amplitude at resonance frequency $\omega_{\text{res}} = \sqrt{K/M}$.

\textbf{Design:} Deliver persuasive messages at varying intervals (e.g., every 30 seconds, 2 minutes, 5 minutes, 10 minutes) across conditions, holding total exposure constant. Measure final belief change amplitude.

\textbf{Discrimination:} The framework predicts a non-monotonic relationship with a peak at intermediate frequency determined by participant confidence. Standard models predict monotonic effects of message frequency. Resonance is a signature of second-order dynamics.

\subsubsection{Social Attention and Epistemic Rigidity}

 

\textbf{Prediction:} Agents who receive sustained social attention accumulate epistemic mass, becoming more resistant to belief revision than equally confident but unattended individuals.

 

\textbf{Design:} In a group deliberation paradigm, manipulate attention asymmetry: some participants are designated as "influencers" whose opinions are solicited and displayed to others, while "observers" hold equally strong priors but receive no social attention. After exposure to identical counter-evidence, measure belief change magnitude and relaxation dynamics across conditions.


\textbf{Discrimination:} Standard models predict belief change depends on prior confidence and evidence strength, not social role. The inertial framework predicts that attended-to advisors exhibit smaller belief shifts and longer relaxation times than observers with matched initial confidence, due to the social attention term $\sum_j \beta_{ji} \Lambda_{q_i}$ contributing to effective mass.
 

\subsubsection{Momentum in Economic Expectations}


\textbf{Prediction:} Professional forecasters with high precision should exhibit overshooting when revising expectations following macroeconomic surprises.


\textbf{Design:} Analyze existing forecast revision data \citet{coibion2015} following large, unexpected economic announcements (e.g., surprise interest rate changes, employment shocks). Track individual forecaster trajectories over subsequent revision rounds, conditioning on pre-shock forecast confidence.

 
\textbf{Discrimination:} Standard information models predict gradual, monotonic adjustment toward rational expectations. The inertial framework predicts that high-confidence forecasters may transiently overshoot, revising past the rational benchmark before settling, while low-confidence forecasters adjust monotonically. This pattern, if present in existing panel data, would distinguish momentum-based from friction-based accounts of expectation stickiness.


\subsection{Relation to Existing Models}

Table~\ref{tab:model_comparison} summarizes predictions distinguishing the inertial framework from existing approaches.

\begin{table}[ht]
\centering
\caption{Predictions distinguishing the inertial framework from first-order models.}
\label{tab:model_comparison}
\begin{tabular}{lcc}
\hline
\textbf{Phenomenon} & \textbf{First-Order Models} & \textbf{This Framework} \\
\hline
Approach to equilibrium & Monotonic & Can oscillate \\
Precision dependence & Weights evidence & Determines inertia \\
Overshooting & Not predicted & Predicted (underdamped) \\
Resonance to periodic input & Not predicted & Predicted \\
Belief perseverance & Separate bias & Emerges from mass \\
Continued influence & Memory failure & Momentum decay \\
Social momentum transfer & Absent & Predicted \\
Attention increases rigidity & Not predicted & Predicted (social mass) \\
Forecast overshooting & Friction/stickiness & Momentum decay \\
\hline
\end{tabular}
\end{table}

Standard Bayesian updating and its neural implementations (predictive coding, active inference) correspond to first-order dissipative dynamics \citet{friston2010, bogacz2017tutorial}. The free energy principle emerges as a limiting case: in the overdamped limit ($\gamma \gg 2\sqrt{KM}$), inertial terms become negligible and dynamics reduce to gradient descent on the free energy landscape. Novel predictions arise when damping is sufficiently weak that second-order terms contribute meaningfully.

The DeGroot model of social learning \citet{degroot1974} and its extensions assume instantaneous opinion averaging without momentum. Our framework predicts that strongly-held beliefs (high $\Lambda$) should resist social influence and potentially induce oscillatory collective dynamics in networks with strong coupling.  These phenomena are absent from standard consensus models but consistent with observed polarization dynamics.

\subsection{Connection to Transformer Attention}

Elsewhere, we have shown that the attention weights $\beta_{ij}$ (Eq.~X) are not merely analogous to transformer self-attention but rather they \emph{are} attention, derived from first principles \citet{Dennis2025, Dennis2025b}. Standard transformers compute $\beta_{ij} = \mathrm{softmax}(q_i^\top W_Q W_K^\top k_j / \sqrt{d})$ using learned projection matrices $W_Q, W_K$ that parameterize an implicit similarity metric. Our framework reveals that this metric is actually the KL divergence on the statistical manifold.

\begin{equation}
\beta_{ij} = \frac{\exp(-D_{\mathrm{KL}}(q_i \| \Omega_{ij}[q_j])/\tau)}{\sum_k \exp(-D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k])/\tau)}
\end{equation}

The gauge transport operators $\Omega_{ij}$ generalize to the role that positional 
encoding and learned projections play in standard architectures. 


\subsection{Hierarchical Extensions}

The microscopic dynamics developed here (belief equilibration timescales $\tau = M/\gamma$, conditions for oscillatory versus monotonic convergence, momentum transfer through attention networks) provide the foundation for multi-scale theories of collective belief. Elsewhere \citet{Dennis2025b}, we show that agents achieving sufficient consensus undergo dynamical renormalization-like coarse-graining, yielding emergent informational meta-agents (e.g. institutions, economies, societies) with their own shared beliefs, precisions, and gauge frames. The present framework supplies the dynamics underlying such emergence.  The conditions under which individual beliefs synchronize, the timescales of collective equilibration, and the momentum currents that drive or resist consensus formation lead to emergent informational hierarchies.

Critically, our theory makes no distinction between "physical" and "abstract" informational systems. The same equations governing individual belief dynamics may similarly apply to institutional beliefs (corporate strategy, scientific consensus, market expectations), with appropriate identification of the relevant state spaces and attention structures.

\subsection{Limitations}

Several simplifying assumptions warrant future relaxation.

\textbf{Gaussian beliefs.} While analytically tractable, Gaussian distributions cannot capture the multi-modal posteriors characteristic of hypothesis competition, cognitive dissonance, or attitude ambivalence. Extension to exponential families is straightforward however, extension to arbitrary distributions requires numerical methods or variational approximations.


\textbf{Quasi-static precision.} We have treated precision $\Lambda$ as slowly-varying relative to the mean $\mu$ for demonstrative purposes. The complete theory couples mean and precision dynamics on $\mathbb{R}^d \times \mathrm{SPD}(d)$, allowing precision itself to oscillate or exhibit inertia. The codebase we have built, found in the repository (below),  supports the full dynamics as well as hierarchical emergence and more. The full details are too rich to confine to a single article.

\textbf{Empirical validation.} Our primary empirical test involved a task designed to elicit overdamped dynamics. Direct observation of underdamped belief oscillation, precision-scaled relaxation, or resonant persuasion remains for future experimental work. The Kaplowitz-Fink results \citet{kaplowitz1983, fink2002} provide suggestive evidence, but replication with publicly available datasets utilizing the present framework's predictions would strengthen the empirical case.



\section{Conclusion}

We have shown that beliefs naturally possess inertia in relation to prior precision. The straightforward identification that epistemic mass equals statistical precision transforms our understanding of belief dynamics provides new tools that extend beyond dissipative gradient flow and into rich Hamiltonian dynamics.

Our theory predicts oscillations, over-shooting, resistance, decay, and resonances in belief dynamics. More fundamentally, it re-frames cognitive biases not as irrationality but instead as unavoidable consequences of belief inertia. Just as physical mass resists acceleration, cognitive precision resists belief change. 

This shift in perspective offers researchers new tools and methods for understanding persuasion, education, therapy, negotiation, and social dynamics. By recognizing that confident beliefs are massive and uncertain beliefs are light, we chart new frontiers in research and socio-psychological understanding.

The mathematics has been hiding in plain sight for decades due to lack of transitive communication between far-flung fields of differential geometry, physics, and informational geometry.  The Fisher information metric has been whispering this entire time that it is actually an inertia tensor for the dynamics of thought.

\subsection*{Data Availability}
All code and data will be made publicly available upon publication at \\ 
https://github.com/cdenn016/Hamiltonian-VFE

\subsection*{Acknowledgments}
Claude Sonnet 4.5 was utilized for programming our variational free energy descent simulation suite.  All code was manually reviewed, corrected, and mathematically validated by the author.  Furthermore, Claude was utilized for typesetting figures, LaTeX equations, and general organizational and manuscript clarity advice. The author further declares no funding or conflicts of interest.

\appendix

%==============================================================================
% APPENDIX B: GAUGE FRAME VARIATIONS AND PULLBACK GEOMETRY
%==============================================================================

\section{Gauge Frame Variations and Pullback Geometry}
\label{app:gauge}

The Hamiltonian formulation of belief dynamics reflects deep geometric structure. Each agent's belief space carries a gauge freedom---the choice of coordinate frame in which beliefs are expressed. Physical quantities must be invariant under these gauge transformations, while the dynamics must be covariant. This appendix develops the transformation theory for the mass matrix, momenta, and Hamilton's equations under gauge frame variations.

%------------------------------------------------------------------------------
\subsection{Gauge Structure of Multi-Agent Belief Systems}
%------------------------------------------------------------------------------

\subsubsection{The Principal Bundle}

The geometric setting is a principal $G$-bundle $\pi: P \to \mathcal{C}$ where $\mathcal{C}$ denotes the base manifold encoding agent positions and social network topology, $G = \mathrm{SO}(d)$ is the gauge group corresponding to rotations in belief space, and the fiber $\pi^{-1}(c)$ over each point $c \in \mathcal{C}$ is the space of reference frames.

Each agent $i$ located at $c_i \in \mathcal{C}$ expresses beliefs in a local frame. The \textbf{transport operator} $\Omega_{ik} \in \mathrm{SO}(d)$ relates agent $k$'s frame to agent $i$'s frame.

\subsubsection{Gauge Transformations}

A \textbf{gauge transformation} is a smooth assignment of group elements to each agent

\begin{equation}
g: \{1, \ldots, N\} \to \mathrm{SO}(d), \quad i \mapsto g_i
\end{equation}

Under this transformation, belief parameters transform as:
\begin{align}
\mu_i &\mapsto \mu_i' = g_i \mu_i \\
\Sigma_i &\mapsto \Sigma_i' = g_i \Sigma_i g_i^T \\
\Lambda_{q_i} &\mapsto \Lambda_{q_i}' = g_i \Lambda_{q_i} g_i^T
\end{align}

The transport operators transform as:
\begin{equation}
\Omega_{ik} \mapsto \Omega_{ik}' = g_i \Omega_{ik} g_k^{-1}
\end{equation}

This ensures that the transported belief $\tilde{q}_k = \Omega_{ik}[q_k]$ transforms consistently:
\begin{equation}
\tilde{\mu}_k' = g_i \tilde{\mu}_k, \quad \tilde{\Lambda}_{q_k}' = g_i \tilde{\Lambda}_{q_k} g_i^T
\end{equation}

%------------------------------------------------------------------------------
\subsection{Transformation of the Mass Matrix}
%------------------------------------------------------------------------------

\subsubsection{Mean Sector}

The mean-sector mass matrix transforms as a tensor under gauge transformations.

\paragraph{Diagonal blocks:}
\begin{align}
[\mathbf{M}^\mu]_{ii}' &= \bar{\Lambda}_{p_i}' + \sum_k \beta_{ik}\tilde{\Lambda}_{q_k}' + \sum_j \beta_{ji}\Lambda_{q_i}' \nonumber\\
&= g_i\bar{\Lambda}_{p_i} g_i^T + \sum_k \beta_{ik} g_i\tilde{\Lambda}_{q_k} g_i^T + \sum_j \beta_{ji} g_i\Lambda_{q_i} g_i^T \nonumber\\
&= g_i \left[\bar{\Lambda}_{p_i} + \sum_k \beta_{ik}\tilde{\Lambda}_{q_k} + \sum_j \beta_{ji}\Lambda_{q_i}\right] g_i^T \nonumber\\
&= g_i \, [\mathbf{M}^\mu]_{ii} \, g_i^T
\end{align}

\paragraph{Off-diagonal blocks:}
\begin{align}
[\mathbf{M}^\mu]_{ik}' &= -\beta_{ik}\Omega_{ik}'\Lambda_{q_k}' - \beta_{ki}\Lambda_{q_i}'(\Omega_{ki}')^T \nonumber\\
&= -\beta_{ik}(g_i\Omega_{ik}g_k^{-1})(g_k\Lambda_{q_k} g_k^T) - \beta_{ki}(g_i\Lambda_{q_i} g_i^T)(g_k\Omega_{ki}g_i^{-1})^T \nonumber\\
&= -\beta_{ik} g_i\Omega_{ik}\Lambda_{q_k} g_k^T - \beta_{ki} g_i\Lambda_{q_i}\Omega_{ki}^T g_k^T \nonumber\\
&= g_i \, [\mathbf{M}^\mu]_{ik} \, g_k^T
\end{align}

\paragraph{Block matrix form:}

Define the block-diagonal gauge matrix:
\begin{equation}
\mathbf{G} = \mathrm{diag}(g_1, g_2, \ldots, g_N) \in \mathrm{SO}(d)^N
\end{equation}

Then the full mean-sector mass matrix transforms as:
\begin{equation}
\boxed{(\mathbf{M}^\mu)' = \mathbf{G} \, \mathbf{M}^\mu \, \mathbf{G}^T}
\end{equation}

This is the transformation law for a $(0,2)$-tensor (metric tensor) on the product manifold.

\subsubsection{Covariance Sector}

The covariance-sector mass involves Kronecker products. Under gauge transformation:
\begin{align}
[\mathbf{M}^\Sigma]_{ii}' &= \frac{1}{2}(\Lambda_{q_i}' \otimes \Lambda_{q_i}') \cdot \left(1 + \sum_k \beta_{ik} + \sum_j \beta_{ji}\right) \nonumber\\
&= \frac{1}{2}(g_i\Lambda_{q_i} g_i^T \otimes g_i\Lambda_{q_i} g_i^T) \cdot \left(1 + \sum_k \beta_{ik} + \sum_j \beta_{ji}\right) \nonumber\\
&= \frac{1}{2}(g_i \otimes g_i)(\Lambda_{q_i} \otimes \Lambda_{q_i})(g_i^T \otimes g_i^T) \cdot \left(1 + \sum_k \beta_{ik} + \sum_j \beta_{ji}\right)
\end{align}

The transformation law is:
\begin{equation}
\boxed{(\mathbf{M}^\Sigma)' = (\mathbf{G} \otimes \mathbf{G}) \, \mathbf{M}^\Sigma \, (\mathbf{G}^T \otimes \mathbf{G}^T)}
\end{equation}

\subsubsection{Cross Blocks}

The mean-covariance cross blocks transform as:
\begin{equation}
(\mathbf{C}^{\mu\Sigma})' = \mathbf{G} \, \mathbf{C}^{\mu\Sigma} \, (\mathbf{G}^T \otimes \mathbf{G}^T)
\end{equation}

%------------------------------------------------------------------------------
\subsection{Transformation of Momenta}
%------------------------------------------------------------------------------

For Hamilton's equations to be covariant, momenta must transform contragrediently to positions.

\subsubsection{Mean Momentum}

The mean momentum transforms as a covector:
\begin{equation}
\boxed{(\pi_i^\mu)' = g_i \, \pi_i^\mu}
\end{equation}

This ensures the pairing $\langle\pi^\mu, \dot{\mu}\rangle$ is gauge-invariant:
\begin{equation}
\langle(\pi^\mu)', \dot{\mu}'\rangle = (g_i\pi_i^\mu)^T(g_i\dot{\mu}_i) = (\pi_i^\mu)^T g_i^T g_i \dot{\mu}_i = (\pi_i^\mu)^T\dot{\mu}_i = \langle\pi^\mu, \dot{\mu}\rangle
\end{equation}

\subsubsection{Covariance Momentum}

The covariance momentum $\Pi^\Sigma \in \mathrm{Sym}(d)$ transforms as:
\begin{equation}
\boxed{(\Pi_i^\Sigma)' = g_i \, \Pi_i^\Sigma \, g_i^T}
\end{equation}

The pairing with $\dot{\Sigma}$ uses the trace:
\begin{equation}
\mathrm{tr}[(\Pi^\Sigma)'\dot{\Sigma}'] = \mathrm{tr}[(g_i\Pi_i^\Sigma g_i^T)(g_i\dot{\Sigma}_i g_i^T)] = \mathrm{tr}[\Pi_i^\Sigma\dot{\Sigma}_i]
\end{equation}
where we used cyclicity of the trace and $g_i^T g_i = I$.

%------------------------------------------------------------------------------
\subsection{Covariance of Hamilton's Equations}
%------------------------------------------------------------------------------

\subsubsection{Velocity Equation}

The velocity equation $\dot{\mu} = (\mathbf{M}^\mu)^{-1}\pi^\mu$ transforms as:
\begin{align}
\dot{\mu}' &= ((\mathbf{M}^\mu)')^{-1}(\pi^\mu)' \nonumber\\
&= (\mathbf{G}\mathbf{M}^\mu\mathbf{G}^T)^{-1}\mathbf{G}\pi^\mu \nonumber\\
&= \mathbf{G}^{-T}(\mathbf{M}^\mu)^{-1}\mathbf{G}^{-1}\mathbf{G}\pi^\mu \nonumber\\
&= \mathbf{G}(\mathbf{M}^\mu)^{-1}\pi^\mu \quad \text{(since } \mathbf{G}^{-T} = \mathbf{G} \text{ for SO}(d)\text{)} \nonumber\\
&= \mathbf{G}\dot{\mu}
\end{align}

This confirms $\dot{\mu}$ transforms as a vector: $\dot{\mu}' = \mathbf{G}\dot{\mu}$.

\subsubsection{Force Equation}

The force equation involves the free energy gradient. Under gauge transformation:
\begin{equation}
\left(\frac{\partial F}{\partial\mu_i}\right)' = g_i \frac{\partial F}{\partial\mu_i}
\end{equation}

This follows from the chain rule and the invariance of $F$ under gauge transformations when transport operators transform consistently.

The geodesic force transforms similarly, ensuring full covariance:
\begin{equation}
\boxed{\dot{\pi}' = \mathbf{G}\dot{\pi}}
\end{equation}

%------------------------------------------------------------------------------
\subsection{The Connection and Its Variation}
%------------------------------------------------------------------------------

\subsubsection{Connection 1-Form}

The transport operators $\Omega_{ik}$ encode a discrete connection on the agent network. For agents connected along an edge $e = (i,k)$, define:
\begin{equation}
A_e = \Omega_{ik} \in \mathrm{SO}(d)
\end{equation}

Under gauge transformation:
\begin{equation}
A_e \mapsto A_e' = g_i A_e g_k^{-1}
\end{equation}

This is the discrete analog of the gauge transformation $A \mapsto gAg^{-1} + g\,dg^{-1}$ for continuous connections.

\subsubsection{Curvature}

The curvature around a closed loop $\gamma = (i \to j \to k \to i)$ is:
\begin{equation}
F_\gamma = \Omega_{ij}\Omega_{jk}\Omega_{ki} \in \mathrm{SO}(d)
\end{equation}

This is gauge-covariant: $F_\gamma' = g_i F_\gamma g_i^{-1}$.

A \textbf{flat connection} satisfies $F_\gamma = I$ for all loops, meaning beliefs can be consistently parallel-transported around any cycle. Nonzero curvature represents ``information geometry frustration''---belief frames cannot be consistently aligned around cycles.

\subsubsection{Variation of Connection}

Consider an infinitesimal variation of the connection:
\begin{equation}
\delta\Omega_{ik} = \omega_{ik} \, \Omega_{ik}, \quad \omega_{ik} \in \mathfrak{so}(d)
\end{equation}

The variation of transported precision is:
\begin{equation}
\delta\tilde{\Lambda}_k = \omega_{ik}\tilde{\Lambda}_k + \tilde{\Lambda}_k\omega_{ik}^T = [\omega_{ik}, \tilde{\Lambda}_k]
\end{equation}
where this equals the commutator since $\omega_{ik}^T = -\omega_{ik}$ (antisymmetry).

%------------------------------------------------------------------------------
\subsection{Variation of the Mass Matrix Under Connection Changes}
%------------------------------------------------------------------------------

\subsubsection{Diagonal Block Variation}

\begin{equation}
\delta[\mathbf{M}^\mu]_{ii} = \sum_k \beta_{ik} \, \delta\tilde{\Lambda}_{q_k} = \sum_k \beta_{ik} \, [\omega_{ik}, \tilde{\Lambda}_{q_k}]
\end{equation}

\subsubsection{Off-Diagonal Block Variation}

\begin{align}
\delta[\mathbf{M}^\mu]_{ik} &= -\beta_{ik} \, \delta(\Omega_{ik}\Lambda_{q_k}) - \beta_{ki} \, \delta(\Lambda_{q_i}\Omega_{ki}^T) \nonumber\\
&= -\beta_{ik}\omega_{ik}\Omega_{ik}\Lambda_{q_k} - \beta_{ki}\Lambda_{q_i}(\omega_{ki}\Omega_{ki})^T \nonumber\\
&= -\beta_{ik}\omega_{ik}\Omega_{ik}\Lambda_{q_k} - \beta_{ki}\Lambda_{q_i}\Omega_{ki}^T\omega_{ki}^T
\end{align}

Using $\omega_{ki}^T = -\omega_{ki}$ (antisymmetry):
\begin{equation}
\boxed{\delta[\mathbf{M}^\mu]_{ik} = -\beta_{ik}\omega_{ik}\Omega_{ik}\Lambda_{q_k} + \beta_{ki}\Lambda_{q_i}\Omega_{ki}^T\omega_{ki}}
\end{equation}

%------------------------------------------------------------------------------
\subsection{Pullback Geometry}
%------------------------------------------------------------------------------

The \textbf{pullback} of the metric under a map $\phi: \mathcal{Q} \to \mathcal{Q}$ is central to understanding how geometry transforms under coordinate changes or symmetry actions.

\subsubsection{Pullback of the Fisher-Rao Metric}

Let $\phi_g: \mathcal{Q} \to \mathcal{Q}$ be the action of gauge transformation $g$:
\begin{equation}
\phi_g(\mu, \Sigma) = (g\mu, g\Sigma g^T)
\end{equation}

The pullback metric is:
\begin{equation}
(\phi_g^*\mathcal{G})_{(\mu,\Sigma)}(v, w) = \mathcal{G}_{\phi_g(\mu,\Sigma)}(d\phi_g \cdot v, d\phi_g \cdot w)
\end{equation}

For the Fisher-Rao metric, gauge invariance implies:
\begin{equation}
\boxed{\phi_g^*\mathcal{G} = \mathcal{G}}
\end{equation}

The metric is \textbf{gauge-invariant}---this is the geometric content of our transformation laws.

\subsubsection{Horizontal and Vertical Decomposition}

The tangent space at each point decomposes as:
\begin{equation}
T_{(\mu,\Sigma)}\mathcal{Q} = H_{(\mu,\Sigma)} \oplus V_{(\mu,\Sigma)}
\end{equation}

The tangent space at each point splits into a \textbf{vertical space} $V$ consisting of directions along gauge orbits (pure gauge changes) and a \textbf{horizontal space} $H$ consisting of directions orthogonal to gauge orbits (physical changes). The connection determines the horizontal subspace. A vector $v = (\delta\mu, \delta\Sigma)$ is horizontal if:
\begin{equation}
\mathcal{G}(v, \xi_X) = 0 \quad \forall X \in \mathfrak{so}(d)
\end{equation}
where $\xi_X$ is the vector field generated by $X$.

\subsubsection{Physical (Gauge-Invariant) Quantities}

Only horizontal components of velocities and momenta correspond to physical observables. Key gauge-invariant quantities include the \textbf{consensus divergence} $\|\mu_i - \tilde{\mu}_k\|_{\tilde{\Lambda}_{q_k}}^2 = (\mu_i - \tilde{\mu}_k)^T\tilde{\Lambda}_{q_k}(\mu_i - \tilde{\mu}_k)$, the \textbf{free energy} $F[\{q_i\}]$ which is gauge-invariant by construction, the \textbf{Hamiltonian} $H = \frac{1}{2}\langle\pi, \mathbf{M}^{-1}\pi\rangle + F$, and the \textbf{inter-agent KL divergence} $\mathrm{KL}(q_i \| \Omega_{ik}[q_k])$.

%------------------------------------------------------------------------------
\subsection{Gauge-Fixed Dynamics}
%------------------------------------------------------------------------------

For numerical implementation, it is often convenient to work in a fixed gauge.

\subsubsection{Identity Gauge}

Set $g_i = I$ for all agents. In this gauge, transport operators $\Omega_{ik}$ directly represent the frame transformations, all quantities take their ``bare'' form, and gauge redundancy is eliminated.

\subsubsection{Consensus-Aligned Gauge}

Choose gauges so that at equilibrium:
\begin{equation}
\Omega_{ik}^* = I \quad \text{(parallel frames at consensus)}
\end{equation}

This simplifies analysis near equilibrium since transported quantities equal untransported ones.

\subsubsection{Principal Axis Gauge}

For each agent, choose $g_i$ to diagonalize $\Sigma_i$:
\begin{equation}
\Sigma_i' = g_i\Sigma_i g_i^T = \mathrm{diag}(\lambda_1^{(i)}, \ldots, \lambda_d^{(i)})
\end{equation}

This separates dynamics along principal axes of uncertainty.

%------------------------------------------------------------------------------
\subsection{Summary: Gauge-Covariant Hamiltonian Mechanics}
%------------------------------------------------------------------------------

\begin{tcolorbox}[title=Gauge Transformation Laws]
\textbf{Positions:}
\begin{align}
\mu_i' &= g_i\mu_i & \Sigma_i' &= g_i\Sigma_i g_i^T
\end{align}

\textbf{Momenta:}
\begin{align}
(\pi_i^\mu)' &= g_i\pi_i^\mu & (\Pi_i^\Sigma)' &= g_i\Pi_i^\Sigma g_i^T
\end{align}

\textbf{Mass Matrix:}
\begin{equation}
\mathbf{M}' = \mathbf{G}\mathbf{M}\mathbf{G}^T
\end{equation}

\textbf{Transport Operators:}
\begin{equation}
\Omega_{ik}' = g_i\Omega_{ik}g_k^{-1}
\end{equation}

\textbf{Hamilton's Equations:} Fully covariant under these transformations.

\textbf{Physical Observables:} Gauge-invariant quantities include $F$, $H$, and all inter-agent divergences.
\end{tcolorbox}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix: Hamiltonian Mechanics on Statistical Manifolds
% With Explicit Sensory Likelihood
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hamiltonian Mechanics on Statistical Manifolds}
\label{app:hamiltonian}

This appendix derives the complete mass matrix structure for multi-agent belief dynamics with explicit sensory evidence, demonstrating that inertial mass emerges as statistical precision. We work in the quasi-static approximation where prior parameters $(\bar{\mu}_i, \bar{\Sigma}_i)$ evolve slowly relative to beliefs $(\mu_i, \Sigma_i)$.

%-----------------------------------------------------------------------
\subsection{Setup and Notation}
%-----------------------------------------------------------------------

Each agent $i$ maintains a belief distribution $q_i = \mathcal{N}(\mu_i, \Sigma_i)$ anchored to a fixed prior $p_i = \mathcal{N}(\bar{\mu}_i, \bar{\Sigma}_i)$ and receives observations $o_i$ through a likelihood $p(o_i \mid \theta) = \mathcal{N}(o_i; \theta, \Sigma_{o_i})$. Define:
%
\begin{align}
\Lambda_{q_i} &= \Sigma_i^{-1} & &\text{(belief precision)} \\
\bar{\Lambda}_{p_i} &= \bar{\Sigma}_i^{-1} & &\text{(prior precision)} \\
\Lambda_{o_i} &= \Sigma_{o_i}^{-1} & &\text{(observation precision)} \\
\tilde{\mu}_k &= \Omega_{ik}\mu_k & &\text{(transported mean)} \\
\tilde{\Lambda}_{q_k} &= \Omega_{ik}\Lambda_{q_k}\Omega_{ik}^T & &\text{(transported precision)}
\end{align}
%
where $\Omega_{ik} \in \mathrm{SO}(d)$ is the gauge transport operator from agent $k$'s frame to agent $i$'s frame, given by $\Omega_{ik} = e^{\phi_i}e^{-\phi_k}$ with $\phi_i \in \mathfrak{so}(d)$.

%-----------------------------------------------------------------------
\subsection{The Extended Free Energy Functional}
%-----------------------------------------------------------------------

The complete variational free energy with explicit sensory evidence is:
%
\begin{equation}
\boxed{
\mathcal{F}[\{q_i\}] = \sum_i D_{\mathrm{KL}}(q_i \| p_i) + \sum_{i,k} \beta_{ik} D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k]) - \sum_i \mathbb{E}_{q_i}[\log p(o_i \mid \theta)]
}
\label{eq:full_free_energy}
\end{equation}
%
The three terms represent, respectively, \textbf{prior anchoring} (deviation from internal world-model), \textbf{social consensus} (alignment with other agents via gauge-covariant transport), and \textbf{sensory evidence} (grounding in observations).

%-----------------------------------------------------------------------
\subsection{Component Free Energies for Gaussians}
%-----------------------------------------------------------------------

\subsubsection{KL Divergence Between Gaussians}

For $q = \mathcal{N}(\mu_q, \Sigma_q)$ and $p = \mathcal{N}(\mu_p, \Sigma_p)$:
%
\begin{equation}
D_{\mathrm{KL}}(q \| p) = \frac{1}{2}\left[\mathrm{tr}(\Sigma_p^{-1}\Sigma_q) + (\mu_p - \mu_q)^T\Sigma_p^{-1}(\mu_p - \mu_q) - d + \ln\frac{|\Sigma_p|}{|\Sigma_q|}\right]
\label{eq:kl_gaussian}
\end{equation}

\subsubsection{Expected Log-Likelihood}

For the Gaussian likelihood $p(o_i \mid \theta) = \mathcal{N}(o_i; \theta, \Sigma_{o_i})$:
%
\begin{align}
\mathbb{E}_{q_i}[\log p(o_i \mid \theta)] &= -\frac{d}{2}\log(2\pi) - \frac{1}{2}\log|\Sigma_{o_i}| - \frac{1}{2}\mathbb{E}_{q_i}\left[(o_i - \theta)^T\Lambda_{o_i}(o_i - \theta)\right]
\end{align}
%
The quadratic expectation evaluates to:
%
\begin{equation}
\mathbb{E}_{q_i}\left[(o_i - \theta)^T\Lambda_{o_i}(o_i - \theta)\right] = (o_i - \mu_i)^T\Lambda_{o_i}(o_i - \mu_i) + \mathrm{tr}(\Lambda_{o_i}\Sigma_i)
\end{equation}
%
Therefore:
%
\begin{equation}
\boxed{
-\mathbb{E}_{q_i}[\log p(o_i \mid \theta)] = \frac{1}{2}(o_i - \mu_i)^T\Lambda_{o_i}(o_i - \mu_i) + \frac{1}{2}\mathrm{tr}(\Lambda_{o_i}\Sigma_i) + \mathrm{const}
}
\label{eq:neg_log_likelihood}
\end{equation}

%-----------------------------------------------------------------------
\subsection{First Variations (Gradient)}
%-----------------------------------------------------------------------

\subsubsection{Prior Term: $D_{\mathrm{KL}}(q_i \| p_i)$}

\begin{align}
\frac{\partial D_{\mathrm{KL}}(q_i \| p_i)}{\partial \mu_i} &= \bar{\Lambda}_{p_i}(\mu_i - \bar{\mu}_i) \label{eq:grad_prior_mu} \\[6pt]
\frac{\partial D_{\mathrm{KL}}(q_i \| p_i)}{\partial \Sigma_i} &= \frac{1}{2}(\bar{\Lambda}_{p_i} - \Lambda_{q_i}) \label{eq:grad_prior_sigma}
\end{align}

\subsubsection{Consensus Term: $D_{\mathrm{KL}}(q_i \| \tilde{q}_k)$}

With respect to receiver $i$:
\begin{align}
\frac{\partial D_{\mathrm{KL}}(q_i \| \tilde{q}_k)}{\partial \mu_i} &= \tilde{\Lambda}_{q_k}(\mu_i - \tilde{\mu}_k) \label{eq:grad_consensus_mu_i} \\[6pt]
\frac{\partial D_{\mathrm{KL}}(q_i \| \tilde{q}_k)}{\partial \Sigma_i} &= \frac{1}{2}(\tilde{\Lambda}_{q_k} - \Lambda_{q_i}) \label{eq:grad_consensus_sigma_i}
\end{align}

With respect to sender $k$:
\begin{align}
\frac{\partial D_{\mathrm{KL}}(q_i \| \tilde{q}_k)}{\partial \mu_k} &= \Lambda_{q_k}\Omega_{ik}^T(\tilde{\mu}_k - \mu_i) \label{eq:grad_consensus_mu_k} \\[6pt]
\frac{\partial D_{\mathrm{KL}}(q_i \| \tilde{q}_k)}{\partial \Sigma_k} &= \frac{1}{2}\Omega_{ik}^T\left[\tilde{\Lambda}_{q_k} - \tilde{\Lambda}_{q_k}\Sigma_i\tilde{\Lambda}_{q_k}\right]\Omega_{ik} \label{eq:grad_consensus_sigma_k}
\end{align}

\subsubsection{Sensory Term: $-\mathbb{E}_{q_i}[\log p(o_i \mid \theta)]$}

\begin{align}
\frac{\partial}{\partial \mu_i}\left[-\mathbb{E}_{q_i}[\log p(o_i \mid \theta)]\right] &= \Lambda_{o_i}(\mu_i - o_i) \label{eq:grad_sensory_mu} \\[6pt]
\frac{\partial}{\partial \Sigma_i}\left[-\mathbb{E}_{q_i}[\log p(o_i \mid \theta)]\right] &= \frac{1}{2}\Lambda_{o_i} \label{eq:grad_sensory_sigma}
\end{align}

\subsubsection{Total Gradient}

\begin{equation}
\boxed{
\frac{\partial \mathcal{F}}{\partial \mu_i} = \bar{\Lambda}_{p_i}(\mu_i - \bar{\mu}_i) + \sum_k \beta_{ik}\tilde{\Lambda}_{q_k}(\mu_i - \tilde{\mu}_k) + \sum_j \beta_{ji}\Lambda_{q_i}\Omega_{ji}^T(\tilde{\mu}_i^{(j)} - \mu_j) + \Lambda_{o_i}(\mu_i - o_i)
}
\label{eq:total_gradient_mu}
\end{equation}
%
where $\tilde{\mu}_i^{(j)} = \Omega_{ji}\mu_i$ is agent $i$'s mean transported into agent $j$'s frame.

\begin{equation}
\frac{\partial \mathcal{F}}{\partial \Sigma_i} = \frac{1}{2}(\bar{\Lambda}_{p_i} - \Lambda_{q_i}) + \sum_k \frac{\beta_{ik}}{2}(\tilde{\Lambda}_{q_k} - \Lambda_{q_i}) + \sum_j \frac{\beta_{ji}}{2}\Omega_{ji}^T\left[\tilde{\Lambda}_{qi}^{(j)} - \tilde{\Lambda}_{qi}^{(j)}\Sigma_j\tilde{\Lambda}_{qi}^{(j)}\right]\Omega_{ji} + \frac{1}{2}\Lambda_{o_i}
\label{eq:total_gradient_sigma}
\end{equation}

%-----------------------------------------------------------------------
\subsection{Second Variations (Hessian = Mass Matrix)}
%-----------------------------------------------------------------------

The Hessian $\mathbf{M} = \partial^2\mathcal{F}/\partial\xi\partial\xi$ serves as the mass matrix. This is distinct from the intrinsic Fisher-Rao metric on the statistical manifold; it is a ``Hessian mass matrix'' whose curvature depends on the full free energy landscape including priors, sensory evidence, and social coupling.

\subsubsection{Mean Sector: $\partial^2\mathcal{F}/\partial\mu\partial\mu^T$}

\paragraph{Diagonal blocks $(i = k)$:}

From prior:
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_i \| p_i)}{\partial \mu_i \partial \mu_i^T} = \bar{\Lambda}_{p_i}
\end{equation}

From consensus (as receiver):
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_i \| \tilde{q}_k)}{\partial \mu_i \partial \mu_i^T} = \tilde{\Lambda}_{q_k} = \Omega_{ik}\Lambda_{q_k}\Omega_{ik}^T
\end{equation}

From consensus (as sender to agent $j$):
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_j \| \tilde{q}_i)}{\partial \mu_i \partial \mu_i^T} = \Omega_{ji}^T\tilde{\Lambda}_{qi}^{(j)}\Omega_{ji} = \Lambda_{q_i}
\end{equation}

From sensory evidence:
\begin{equation}
\frac{\partial^2}{\partial \mu_i \partial \mu_i^T}\left[-\mathbb{E}_{q_i}[\log p(o_i \mid \theta)]\right] = \Lambda_{o_i}
\end{equation}

\textbf{Total diagonal mass:}
\begin{equation}
\boxed{
[\mathbf{M}^\mu]_{ii} = \underbrace{\bar{\Lambda}_{p_i}}_{\text{prior}} + \underbrace{\sum_k \beta_{ik}\tilde{\Lambda}_{q_k}}_{\text{incoming social}} + \underbrace{\sum_j \beta_{ji}\Lambda_{q_i}}_{\text{outgoing recoil}} + \underbrace{\Lambda_{o_i}}_{\text{sensory}}
}
\label{eq:mass_diagonal}
\end{equation}

\paragraph{Off-diagonal blocks $(i \neq k)$:}

From $D_{\mathrm{KL}}(q_i \| \tilde{q}_k)$:
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_i \| \tilde{q}_k)}{\partial \mu_i \partial \mu_k^T} = -\tilde{\Lambda}_{q_k}\Omega_{ik} = -\Omega_{ik}\Lambda_{q_k}
\end{equation}

From $D_{\mathrm{KL}}(q_k \| \tilde{q}_i)$ (if $k$ also listens to $i$):
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_k \| \tilde{q}_i)}{\partial \mu_i \partial \mu_k^T} = -\Lambda_{q_i}\Omega_{ki}^T
\end{equation}

The sensory term does not couple different agents. Therefore:
\begin{equation}
\boxed{
[\mathbf{M}^\mu]_{ik} = -\beta_{ik}\Omega_{ik}\Lambda_{q_k} - \beta_{ki}\Lambda_{q_i}\Omega_{ki}^T \quad (i \neq k)
}
\label{eq:mass_offdiagonal}
\end{equation}


By Schwarz's theorem, this Hessian is symmetric: one can verify that $[\mathbf{M}^\mu]_{ik} = ([\mathbf{M}^\mu]_{ki})^T$ holds for any $\beta_{ik}$ and $\Omega_{ik}$.


%-----------------------------------------------------------------------
\subsubsection{Covariance Sector: $\partial^2\mathcal{F}/\partial\Sigma\partial\Sigma$}
%-----------------------------------------------------------------------

For matrix-valued variables, we use the directional derivative convention:
\begin{equation}
\frac{\partial^2 f}{\partial \Sigma \partial \Sigma}[V, W] = \lim_{\epsilon \to 0} \frac{1}{\epsilon}\left(\left.\frac{\partial f}{\partial \Sigma}\right|_{\Sigma + \epsilon W} - \left.\frac{\partial f}{\partial \Sigma}\right|_{\Sigma}\right)[V]
\end{equation}

\paragraph{Key identity:}
\begin{equation}
\frac{\partial}{\partial \Sigma}(\Sigma^{-1}) = -\Sigma^{-1} \otimes \Sigma^{-1}
\end{equation}

\paragraph{Diagonal blocks $(i = k)$:}

From prior:
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_i \| p_i)}{\partial \Sigma_i \partial \Sigma_i}[V, W] = \frac{1}{2}\mathrm{tr}\left[\Lambda_{q_i}V\Lambda_{q_i}W\right]
\end{equation}

In tensor notation:
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_i \| p_i)}{\partial \Sigma_i \partial \Sigma_i} = \frac{1}{2}(\Lambda_{q_i} \otimes \Lambda_{q_i})
\end{equation}

From consensus (as receiver and sender), identical contributions arise.

\textbf{Critical observation:} The sensory term $\frac{1}{2}\mathrm{tr}(\Lambda_{o_i}\Sigma_i)$ is \textit{linear} in $\Sigma_i$, so its second derivative \textbf{vanishes}:
\begin{equation}
\frac{\partial^2}{\partial \Sigma_i \partial \Sigma_i}\left[\mathrm{tr}(\Lambda_{o_i}\Sigma_i)\right] = 0
\end{equation}

Therefore:
\begin{equation}
\boxed{
[\mathbf{M}^\Sigma]_{ii} = \frac{1}{2}(\Lambda_{q_i} \otimes \Lambda_{q_i}) \cdot \left(1 + \sum_k \beta_{ik} + \sum_j \beta_{ji}\right)
}
\label{eq:mass_covariance}
\end{equation}

The sensory precision $\Lambda_{o_i}$ does \textbf{not} contribute to the covariance-sector mass.

%-----------------------------------------------------------------------
\subsubsection{Mean-Covariance Cross Blocks}
%-----------------------------------------------------------------------

\paragraph{Prior term:}
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_i \| p_i)}{\partial \mu_i \partial \Sigma_i} = 0
\end{equation}

\paragraph{Sensory term:} The sensory free energy decomposes into a component quadratic in $\mu_i$, namely $(o_i - \mu_i)^T\Lambda_{o_i}(o_i - \mu_i)$, and a component linear in $\Sigma_i$, namely $\mathrm{tr}(\Lambda_{o_i}\Sigma_i)$. These are independent, so:
\begin{equation}
[\mathbf{C}^{\mu\Sigma}]_{ii}^{\text{sensory}} = 0
\end{equation}

\paragraph{Consensus (cross-agent):} From $\partial D_{\mathrm{KL}}(q_i \| \tilde{q}_k)/\partial \mu_i = \tilde{\Lambda}_{q_k}(\mu_i - \tilde{\mu}_k)$, varying $\Sigma_k$:
\begin{equation}
\frac{\partial^2 D_{\mathrm{KL}}(q_i \| \tilde{q}_k)}{\partial \mu_i \partial \Sigma_k}[V] = -\Omega_{ik}\Lambda_{q_k}V\Lambda_{q_k}\Omega_{ik}^T(\mu_i - \tilde{\mu}_k)
\end{equation}

This vanishes at consensus ($\mu_i = \tilde{\mu}_k$):
\begin{equation}
[\mathbf{C}^{\mu\Sigma}]_{ik} = 0 \quad \text{when } \mu_i = \tilde{\mu}_k
\end{equation}

%-----------------------------------------------------------------------
\subsection{Complete Mass Matrix Assembly}
%-----------------------------------------------------------------------

The full state vector is $\xi = (\mu_1, \ldots, \mu_N, \Sigma_1, \ldots, \Sigma_N)$.

\subsubsection{Block Structure}

\begin{equation}
\mathbf{M} = \begin{pmatrix} \mathbf{M}^\mu & \mathbf{C}^{\mu\Sigma} \\ (\mathbf{C}^{\mu\Sigma})^T & \mathbf{M}^\Sigma \end{pmatrix}
\end{equation}
where each block is an $N \times N$ matrix of sub-blocks.

\subsubsection{Explicit Formulae}

\paragraph{Mean sector diagonal:}
\begin{equation}
[\mathbf{M}^\mu]_{ii} = \underbrace{\bar{\Lambda}_{p_i}}_{\text{prior anchoring}} + \underbrace{\sum_k \beta_{ik}\Omega_{ik}\Lambda_{q_k}\Omega_{ik}^T}_{\text{incoming consensus}} + \underbrace{\sum_j \beta_{ji}\Lambda_{q_i}}_{\text{outgoing recoil}} + \underbrace{\Lambda_{o_i}}_{\text{sensory grounding}}
\label{eq:mass_full_diagonal}
\end{equation}

\paragraph{Mean sector off-diagonal:}
\begin{equation}
[\mathbf{M}^\mu]_{ik} = -\beta_{ik}\Omega_{ik}\Lambda_{q_k} - \beta_{ki}\Lambda_{q_i}\Omega_{ki}^T \quad (i \neq k)
\end{equation}

\paragraph{Covariance sector diagonal:}
\begin{equation}
[\mathbf{M}^\Sigma]_{ii} = \frac{1}{2}(\Lambda_{q_i} \otimes \Lambda_{q_i}) \cdot \left(1 + \sum_k \beta_{ik} + \sum_j \beta_{ji}\right)
\end{equation}

\paragraph{Cross mean-covariance (at consensus):}
\begin{equation}
[\mathbf{C}^{\mu\Sigma}]_{ik} = 0 \quad \text{when } \mu_i = \tilde{\mu}_k
\end{equation}

%-----------------------------------------------------------------------
\subsection{Physical Interpretation}
%-----------------------------------------------------------------------

\subsubsection{Mass as Precision}

The mean-sector effective mass for agent $i$ is:
\begin{equation}
\boxed{
M_i = \bar{\Lambda}_{p_i} + \sum_k \beta_{ik}\tilde{\Lambda}_{q_k} + \sum_j \beta_{ji}\Lambda_{q_i} + \Lambda_{o_i}
}
\label{eq:effective_mass}
\end{equation}

The term $\bar{\Lambda}_{p_i}$ represents \textbf{bare mass}, providing inertia against deviation from the prior. The term $\sum_k \beta_{ik}\tilde{\Lambda}_{q_k}$ represents \textbf{incoming relational mass}, the inertia from being ``pulled'' by neighbors. The term $\sum_j \beta_{ji}\Lambda_{q_i}$ represents \textbf{outgoing relational mass}, the inertia from ``pulling'' neighbors (recoil). Finally, $\Lambda_{o_i}$ represents \textbf{sensory mass}, the inertia from grounding in observations.

\subsubsection{Asymmetry of Sensory Contribution}

The sensory precision $\Lambda_{o_i}$ contributes to the mean-sector mass (Eq.~\ref{eq:mass_full_diagonal}) and the mean-sector force (Eq.~\ref{eq:total_gradient_mu}), but \textbf{not} to the covariance-sector mass (Eq.~\ref{eq:mass_covariance}). This asymmetry arises because the sensory term is quadratic in $\mu$ but only linear in $\Sigma$.

\subsubsection{Kinetic Energy}

\begin{equation}
T = \frac{1}{2}\dot{\mu}^T\mathbf{M}^\mu\dot{\mu} + \frac{1}{2}\mathrm{tr}\left[\mathbf{M}^\Sigma[\dot{\Sigma}, \dot{\Sigma}]\right]
\end{equation}

The first term gives standard ``particle'' kinetic energy with precision-mass. The second gives ``shape'' kinetic energy on the SPD manifold; the metric $(\Lambda \otimes \Lambda)$ corresponds to the affine-invariant metric on the cone of symmetric positive-definite matrices.

%-----------------------------------------------------------------------
\subsection{The Hamiltonian}
%-----------------------------------------------------------------------

With conjugate momenta $\pi = (\pi^\mu, \Pi^\Sigma)$ and Hamiltonian:
\begin{equation}
\boxed{
H = \frac{1}{2}\langle\pi, \mathbf{M}^{-1}\pi\rangle + \mathcal{F}[\xi]
}
\label{eq:hamiltonian}
\end{equation}

%-----------------------------------------------------------------------
\subsection{Hamilton's Equations}
%-----------------------------------------------------------------------

\subsubsection{Equations of Motion}

\begin{align}
\dot{\mu}_i &= \sum_k [\mathbf{M}^{-1}]_{ik}^{\mu\mu}\pi_k^\mu + \sum_k [\mathbf{M}^{-1}]_{ik}^{\mu\Sigma}\Pi_k^\Sigma \label{eq:hamilton_mu} \\[6pt]
\dot{\Sigma}_i &= \sum_k [\mathbf{M}^{-1}]_{ik}^{\Sigma\mu}\pi_k^\mu + \sum_k [\mathbf{M}^{-1}]_{ik}^{\Sigma\Sigma}\Pi_k^\Sigma \label{eq:hamilton_sigma} \\[6pt]
\dot{\pi}_i^\mu &= -\frac{\partial \mathcal{F}}{\partial \mu_i} - \frac{1}{2}\pi^T\frac{\partial \mathbf{M}^{-1}}{\partial \mu_i}\pi \label{eq:hamilton_pi_mu} \\[6pt]
\dot{\Pi}_i^\Sigma &= -\frac{\partial \mathcal{F}}{\partial \Sigma_i} - \frac{1}{2}\pi^T\frac{\partial \mathbf{M}^{-1}}{\partial \Sigma_i}\pi \label{eq:hamilton_pi_sigma}
\end{align}

\subsubsection{Force Decomposition}

The potential forces decompose into four physically distinct contributions:
\begin{equation}
\boxed{
-\frac{\partial \mathcal{F}}{\partial \mu_i} = \underbrace{-\bar{\Lambda}_{p_i}(\mu_i - \bar{\mu}_i)}_{\text{prior restoring}} \underbrace{- \sum_k \beta_{ik}\tilde{\Lambda}_{q_k}(\mu_i - \tilde{\mu}_k)}_{\text{consensus}} \underbrace{- \sum_j \beta_{ji}\Lambda_{q_i}\Omega_{ji}^T(\tilde{\mu}_i^{(j)} - \mu_j)}_{\text{reciprocal}} \underbrace{- \Lambda_{o_i}(\mu_i - o_i)}_{\text{sensory evidence}}
}
\label{eq:force_decomposition}
\end{equation}

The geodesic force $f_i^{\text{geo}} = -\frac{1}{2}\sum_{jkl}(\pi_j^\mu)^T\frac{\partial[\mathbf{M}^{-1}]_{jk}^{\mu\mu}}{\partial \mu_i}\pi_k^\mu$ encodes manifold curvature.

\subsubsection{Compact Form}

\begin{equation}
\boxed{
\begin{aligned}
\dot{\xi} &= \mathbf{M}^{-1}\pi \\
\dot{\pi} &= -\nabla\mathcal{F} - \frac{1}{2}\nabla_\xi\langle\pi, \mathbf{M}^{-1}\pi\rangle
\end{aligned}
}
\end{equation}

with $dH/dt = 0$ along trajectories. Since $\mathbf{M}$ depends on $\Sigma$, the Hamiltonian is non-separable, requiring implicit or splitting methods for symplectic integration.

%-----------------------------------------------------------------------
\subsection{Damped Dynamics}
%-----------------------------------------------------------------------

Including dissipation yields:
\begin{equation}
M_i\ddot{\mu}_i + \gamma_i\dot{\mu}_i + \nabla_{\mu_i}\mathcal{F} = 0
\label{eq:damped_dynamics}
\end{equation}

For small displacements from equilibrium with stiffness $K_i = \nabla^2\mathcal{F}|_{\mu^*}$:
\begin{equation}
M_i\ddot{\delta\mu} + \gamma_i\dot{\delta\mu} + K_i\delta\mu = 0
\end{equation}

The discriminant $\Delta = \gamma_i^2 - 4K_iM_i$ determines three regimes: the \textbf{overdamped} regime ($\Delta > 0$) exhibits monotonic decay corresponding to standard Bayesian updating; the \textbf{critically damped} regime ($\Delta = 0$) achieves fastest equilibration; and the \textbf{underdamped} regime ($\Delta < 0$) exhibits oscillatory approach with overshooting.

%-----------------------------------------------------------------------
\subsection{Momentum Current with Sensory Coupling}
%-----------------------------------------------------------------------

Between agents, the momentum current is:
\begin{equation}
J_{k \to i} = \beta_{ik}\tilde{\Lambda}_{q_k}(\tilde{\mu}_k - \mu_i)
\end{equation}

The continuity equation becomes:
\begin{equation}
\dot{\pi}_i + \gamma_i\dot{\mu}_i + \bar{\Lambda}_{p_i}(\mu_i - \bar{\mu}_i) + \Lambda_{o_i}(\mu_i - o_i) = \sum_k J_{k \to i}
\end{equation}

The sensory term $\Lambda_{o_i}(\mu_i - o_i)$ acts as an additional ``anchoring force'' that grounds the agent in observations, distinct from the social momentum currents.

%-----------------------------------------------------------------------
\subsection{Summary}
%-----------------------------------------------------------------------

\begin{tcolorbox}[colback=gray!5,colframe=gray!75,title=The Complete Theory with Sensory Evidence]

\textbf{State:} Each agent $i$ has belief $q_i = \mathcal{N}(\mu_i, \Sigma_i)$ with fixed prior $p_i = \mathcal{N}(\bar{\mu}_i, \bar{\Sigma}_i)$ and observations $o_i$ with precision $\Lambda_{o_i}$.

\textbf{Free Energy:}
\begin{equation}
\mathcal{F} = \sum_i D_{\mathrm{KL}}(q_i \| p_i) + \sum_{i,k} \beta_{ik} D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k]) - \sum_i \mathbb{E}_{q_i}[\log p(o_i \mid \theta)]
\end{equation}

\textbf{Mass Matrix:}
\begin{equation}
\mathbf{M} = \frac{\partial^2 \mathcal{F}}{\partial \xi \partial \xi} = \text{Hessian of free energy} = \text{Precision}
\end{equation}

\textbf{Effective Mass:}
\begin{equation}
M_i = \bar{\Lambda}_{p_i} + \sum_k \beta_{ik}\tilde{\Lambda}_{q_k} + \sum_j \beta_{ji}\Lambda_{q_i} + \Lambda_{o_i}
\end{equation}

\textbf{Dynamics:}
\begin{equation}
\dot{\xi} = \mathbf{M}^{-1}\pi, \qquad \dot{\pi} = -\nabla\mathcal{F} - \frac{1}{2}\nabla_\xi\langle\pi, \mathbf{M}^{-1}\pi\rangle
\end{equation}

\textbf{Physical Meaning:} Position $\mu_i$ represents what agent $i$ believes; momentum $\pi_i$ represents the rate of belief change times precision; mass equals precision (tight beliefs are heavy); and force represents the pull toward prior, consensus, and observations.

\end{tcolorbox}

%==============================================================================
% APPENDIX: DERIVATION OF SOFTMAX ATTENTION FROM MAXIMUM ENTROPY
%==============================================================================

\section{Derivation of Softmax Attention from Maximum Entropy}
\label{app:softmax}

The softmax attention weights $\beta_{ij}$ are not arbitrary but emerge from a principled maximum entropy argument. We seek attention weights that satisfy a constraint on expected dissimilarity while being maximally uncertain otherwise.

\subsection{The Maximum Entropy Problem}

For agent $i$, let $\beta_{ij}$ denote the attention weight assigned to neighbor $j$, subject to the constraint $\sum_j \beta_{ij} = 1$. We seek the distribution over attention that maximizes entropy subject to a constraint on expected belief dissimilarity:
\begin{equation}
\max_{\{\beta_{ij}\}} \left[ -\sum_j \beta_{ij} \log \beta_{ij} \right] \quad \text{subject to} \quad \sum_j \beta_{ij} \, d_{ij}^2 = C_i, \quad \sum_j \beta_{ij} = 1
\end{equation}
where $d_{ij}^2 = \|\mu_i - \mu_j\|^2_{\Lambda}$ is the precision-weighted squared distance between beliefs.

\subsection{Lagrangian Solution}

Introducing Lagrange multipliers $\lambda$ and $\nu$, the Lagrangian is:
\begin{equation}
\mathcal{L} = -\sum_j \beta_{ij} \log \beta_{ij} - \lambda \left(\sum_j \beta_{ij} d_{ij}^2 - C_i\right) - \nu \left(\sum_j \beta_{ij} - 1\right)
\end{equation}

Taking the derivative with respect to $\beta_{ik}$ and setting it to zero:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \beta_{ik}} = -\log \beta_{ik} - 1 - \lambda d_{ik}^2 - \nu = 0
\end{equation}

Solving for $\beta_{ik}$:
\begin{equation}
\beta_{ik} = \exp\left(-1 - \nu - \lambda d_{ik}^2\right) = \frac{\exp(-\lambda d_{ik}^2)}{Z_i}
\end{equation}
where $Z_i = \sum_j \exp(-\lambda d_{ij}^2)$ is the normalizing partition function determined by the constraint $\sum_j \beta_{ij} = 1$.

\subsection{Interpretation}

The result is exactly the softmax (or Gibbs/Boltzmann) distribution over attention:
\begin{equation}
\boxed{\beta_{ij} = \frac{\exp(-d_{ij}^2 / \kappa)}{\sum_k \exp(-d_{ik}^2 / \kappa)}}
\end{equation}
where $\kappa = 1/\lambda$ is an ``attention temperature'' controlling how sharply attention concentrates on similar neighbors. In the limit $\kappa \to 0$, attention becomes deterministic (winner-take-all), while $\kappa \to \infty$ yields uniform attention regardless of similarity. This derivation shows that softmax attention is not merely a convenient mathematical choice but the unique solution that maximizes entropy subject to a constraint on expected dissimilarity. The same principle underlies the Boltzmann distribution in statistical mechanics and the exponential family in statistics.

%==============================================================================
% APPENDIX: WHY FORWARD KL DIVERGENCE
%==============================================================================

\section{Why Forward KL Divergence?}
\label{app:forward_kl}

A natural question arises: why use the forward KL divergence $D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k])$ rather than the reverse $D_{\mathrm{KL}}(\Omega_{ik}[q_k] \| q_i)$ in the social coupling term? This appendix provides both intuitive and formal justifications.

\subsection{Intuitive Argument: Zero-Forcing vs.\ Mean-Seeking}

The forward KL $D_{\mathrm{KL}}(q \| p)$ is ``mean-seeking'' or ``moment-matching'': it penalizes $q$ for placing mass where $p$ has little mass, encouraging $q$ to cover the support of $p$. The reverse KL $D_{\mathrm{KL}}(p \| q)$ is ``mode-seeking'' or ``zero-forcing'': it penalizes $q$ for failing to place mass where $p$ has mass, encouraging $q$ to concentrate on modes of $p$.

For social influence, the forward direction is more natural. Agent $i$ should be penalized for holding beliefs that disagree with transported neighbor beliefs, regardless of how uncertain those neighbors are. The forward KL achieves this: if agent $i$ is certain about something that neighbor $k$ (in $i$'s frame) considers unlikely, $i$ pays a high cost.

\subsection{Formal Uniqueness Argument}

Consider the general family of $f$-divergences $D_f(q \| p) = \int p(x) f(q(x)/p(x)) dx$ for convex $f$ with $f(1) = 0$. We seek divergences satisfying the following desiderata for social coupling:

\paragraph{Desideratum 1: Convexity in $q$.} The coupling term should be convex in agent $i$'s belief $q_i$, ensuring a well-defined optimization landscape. All $f$-divergences satisfy this when the first argument varies.

\paragraph{Desideratum 2: Additivity under transport.} Under the gauge transport $\Omega_{ik}$, the divergence should decompose sensibly. For Gaussians, this requires the divergence to depend on beliefs only through sufficient statistics $(\mu, \Sigma)$.

\paragraph{Desideratum 3: Fisher information as Hessian.} The Hessian of the coupling term at $q_i = \Omega_{ik}[q_k]$ should recover the Fisher information metric, ensuring consistency with the mass interpretation. Only the forward KL satisfies this: for $D_{\mathrm{KL}}(q \| p)$, we have $\nabla^2_q D_{\mathrm{KL}}(q \| p)\big|_{q=p} = \mathcal{I}_p$, the Fisher information at $p$.

The reverse KL has Hessian $\nabla^2_q D_{\mathrm{KL}}(p \| q)\big|_{q=p} = \mathcal{I}_p$ as well, but crucially, it is not convex in $q$ globally---only locally near $q = p$. This makes the reverse KL unsuitable for the free energy minimization framework, as it can create spurious local minima.

\subsection{Connection to Variational Inference}

In variational inference, the forward KL $D_{\mathrm{KL}}(q \| p)$ is the natural objective when $q$ is the variational approximation and $p$ is the target. Minimizing this drives $q$ toward $p$ in an information-theoretic sense. Our social coupling term $D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k])$ follows this convention: agent $i$'s belief $q_i$ is driven toward the transported neighbor belief $\Omega_{ik}[q_k]$. The reverse direction would have neighbors' beliefs driving toward agent $i$, inverting the causal structure of social influence.

%==============================================================================
% APPENDIX: DERIVATION OF SOCIAL COUPLING FROM NORMALIZED GENERATIVE MODEL
%==============================================================================

\section{Derivation of Social Coupling from Normalized Generative Model}
\label{app:social_coupling}

The social coupling term $\sum_{i,k} \beta_{ik} D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k])$ in our free energy is not an ad hoc regularizer but emerges naturally from a properly normalized generative model with auxiliary agreement variables. This appendix provides the derivation, establishing the coupling as a consequence of gauge-transported Gaussian consistency constraints.

\subsection{Auxiliary Agreement Variables}

To enforce consistency between agents after gauge transport, we introduce an auxiliary ``agreement'' variable for each ordered pair $(i,k)$:
\begin{equation}
z_{ik} \in \mathbb{R}^{d}
\end{equation}
The variable $z_{ik}$ represents what agent $i$ believes agent $k$'s belief looks like after transporting $k$'s belief into $i$'s gauge frame. These auxiliary variables serve as latent mediators that, when integrated out, yield the effective pairwise coupling between agents.

\subsection{The Product-of-Gaussians Construction}

Each agreement variable $z_{ik}$ is drawn from a product of two Gaussians that simultaneously constrain it to match both agent $i$'s belief and the gauge-transported belief of agent $k$:
\begin{equation}
p(z_{ik} \mid \mu_i, \mu_k) \propto \mathcal{N}(z_{ik}; \mu_i, \Lambda_{ik}^{-1}) \cdot \mathcal{N}(z_{ik}; \Omega_{ik}\mu_k, \Lambda_{ik}^{-1})
\end{equation}
where $\Lambda_{ik} \succ 0$ is the alignment precision matrix and $\Omega_{ik} \in \mathrm{SO}(d)$ is the gauge transport operator from agent $k$'s frame to agent $i$'s frame.

Crucially, when we integrate out the agreement variable $z_{ik}$, the normalizing constant depends on $(\mu_i, \mu_k)$. Using the standard result for products of Gaussians:
\begin{equation}
\int \mathcal{N}(z; a, \Sigma) \cdot \mathcal{N}(z; b, \Sigma) \, dz = \mathcal{N}(a; b, 2\Sigma) \propto \exp\left(-\frac{1}{4}(a-b)^\top\Sigma^{-1}(a-b)\right)
\end{equation}

This induces an effective coupling between agents. The marginal prior over belief means, after integrating out all agreement variables, takes the form:
\begin{equation}
p(\{\mu_i\}) \propto \left[\prod_i p_i(\mu_i)\right] \cdot \exp\left(-\frac{1}{4}\sum_{i,k}(\mu_i - \Omega_{ik}\mu_k)^\top\Lambda_{ik}(\mu_i - \Omega_{ik}\mu_k)\right)
\end{equation}
where $p_i(\mu_i) = \mathcal{N}(\mu_i; \bar{\mu}_i, \bar{\Sigma}_i)$ is agent $i$'s local prior. The quadratic terms in the exponential encode pairwise alignment costs, with the factor of $1/4$ arising directly from the Gaussian product formula. This construction yields tractable quadratic potentials, in contrast to general unnormalized Markov random fields where partition functions are intractable.

\subsection{Variational Free Energy}

Under a mean-field posterior approximation $q(\{\mu_i\}) = \prod_i q_i(\mu_i)$ with Gaussian factors $q_i = \mathcal{N}(\mu_i; \mu_{q,i}, \Sigma_{q,i})$, the variational free energy is:
\begin{equation}
\mathcal{F} = \mathbb{E}_q[\log q] - \mathbb{E}_q[\log p(\{\mu_i\})] - \mathbb{E}_q[\log p(o \mid \{\mu_i\})]
\end{equation}

Using the marginal prior from the previous section, we have $-\log p(\{\mu_i\}) = \sum_i [-\log p_i(\mu_i)] + \frac{1}{4}\sum_{i,k}(\mu_i - \Omega_{ik}\mu_k)^\top\Lambda_{ik}(\mu_i - \Omega_{ik}\mu_k) + \text{const}$. Taking expectations:
\begin{equation}
\mathcal{F} = \sum_i D_{\mathrm{KL}}(q_i \| p_i) + \frac{1}{4}\sum_{i,k} \mathbb{E}_{q_i q_k}\left[(\mu_i - \Omega_{ik}\mu_k)^\top \Lambda_{ik} (\mu_i - \Omega_{ik}\mu_k)\right] - \mathbb{E}_q[\log p(o \mid \{\mu_i\})]
\end{equation}

For independent Gaussians, the quadratic expectation evaluates to:
\begin{equation}
\mathbb{E}_{q_i q_k}[(\mu_i - \Omega_{ik}\mu_k)^\top \Lambda_{ik} (\mu_i - \Omega_{ik}\mu_k)] = \mathrm{tr}\left[\Lambda_{ik}(\Sigma_{q,i} + \Omega_{ik}\Sigma_{q,k}\Omega_{ik}^\top)\right] + (\mu_{q,i} - \Omega_{ik}\mu_{q,k})^\top \Lambda_{ik} (\mu_{q,i} - \Omega_{ik}\mu_{q,k})
\end{equation}

\subsection{The Alignment Regime}

We choose the coupling precision to be proportional to the transported neighbor's precision:
\begin{equation}
\Lambda_{ik} := \tau_{ik} (\Omega_{ik}\Sigma_{q,k}\Omega_{ik}^\top)^{-1}
\end{equation}
where $\tau_{ik} > 0$ is a dimensionless coupling strength.

In the alignment regime where the coupling drives beliefs toward consistency, we have $\Sigma_{q,i} \approx \Omega_{ik}\Sigma_{q,k}\Omega_{ik}^\top$. Under this condition, the quadratic expectation becomes proportional to the KL divergence:
\begin{equation}
\frac{1}{4}\mathbb{E}_{q_i q_k}[(\mu_i - \Omega_{ik}\mu_k)^\top \Lambda_{ik} (\mu_i - \Omega_{ik}\mu_k)] \approx \frac{\tau_{ik}}{2} D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k]) + \mathrm{const}
\end{equation}
where the constant absorbs dimension-dependent terms. Defining the normalized attention weight $\beta_{ik} := \tau_{ik}/2$, we obtain the final form of the social coupling term.

\subsection{The Complete Free Energy}

Assembling all terms, the variational free energy takes the form used throughout this paper:
\begin{equation}
\boxed{
\mathcal{F}[\{q_i\}] = \sum_i D_{\mathrm{KL}}(q_i \| p_i) + \sum_{i,k} \beta_{ik} D_{\mathrm{KL}}(q_i \| \Omega_{ik}[q_k]) - \sum_i \mathbb{E}_{q_i}[\log p(o_i \mid \theta)]
}
\end{equation}

This derivation establishes several key points. First, the KL-based social coupling is not an ad hoc choice but emerges from integrating out agreement variables in a normalized generative model. Second, the attention weights $\beta_{ik}$ have a clear interpretation as half the dimensionless coupling strength $\tau_{ik}$. Third, the forward KL direction arises naturally from the construction where agent $i$'s belief is compared against transported neighbor beliefs. Fourth, the gauge transport operators $\Omega_{ik}$ enter through the agreement variable construction, ensuring gauge covariance of the full theory.

The agreement variable construction can be understood intuitively: $z_{ik}$ represents a ``negotiated'' belief state that both agents would accept as consistent. The product-of-Gaussians prior penalizes disagreement between agent $i$'s belief and the transported belief of agent $k$. Integrating out this mediator leaves an effective attraction between the beliefs, weighted by how precisely they are required to agree.


\bibliographystyle{apalike}
\bibliography{references}

\end{document}